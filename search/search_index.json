{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"api_reference/settings/","title":"settings","text":""},{"location":"api_reference/settings/#marvin.settings","title":"<code>marvin.settings</code>","text":""},{"location":"api_reference/settings/#marvin.settings.OpenAISettings","title":"<code>OpenAISettings</code>","text":"<p>Provider-specific settings. Only some of these will be relevant to users.</p>"},{"location":"api_reference/settings/#marvin.settings.Settings","title":"<code>Settings</code>","text":"<p>Marvin settings</p>"},{"location":"api_reference/settings/#marvin.settings.temporary_settings","title":"<code>temporary_settings</code>","text":"<p>Temporarily override Marvin setting values. This will not mutate values that have been already been accessed at module load time.</p> <p>This function should only be used for testing.</p> Example <p>from marvin.settings import settings with temporary_settings(MARVIN_LLM_MAX_TOKENS=100):    assert settings.llm_max_tokens == 100 assert settings.llm_max_tokens == 1500</p>"},{"location":"api_reference/components/ai_application/","title":"ai_application","text":""},{"location":"api_reference/components/ai_application/#marvin.components.ai_application","title":"<code>marvin.components.ai_application</code>","text":""},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.AIApplication","title":"<code>AIApplication</code>","text":"<p>An AI application is a stateful, autonomous, natural language     interface to an application.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Optional[str]</code> <p>The name of the application.</p> <code>description</code> <code>Optional[str]</code> <p>A description of the application.</p> <code>state</code> <code>BaseModel</code> <p>The application's state - this can be any JSON-serializable object.</p> <code>plan</code> <code>AppPlan</code> <p>The AI's plan in service of the application - this can be any JSON-serializable object.</p> <code>tools</code> <code>list[Union[Tool, Callable[..., Any]]]</code> <p>A list of tools that the AI can use to interact with application or outside world.</p> <code>history</code> <code>History</code> <p>A history of all messages sent and received by the AI.</p> <code>additional_prompts</code> <code>list[Prompt]</code> <p>A list of additional prompts that will be added to the prompt stack for rendering.</p> Example <p>Create a simple todo app where AI manages its own state and plan. <pre><code>from marvin import AIApplication\n\ntodo_app = AIApplication(\n    name=\"Todo App\",\n    description=\"A simple todo app.\",\n)\n\ntodo_app(\"I need to go to the store.\")\n\nprint(todo_app.state, todo_app.plan)\n</code></pre></p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.AppPlan","title":"<code>AppPlan</code>","text":"<p>The AI's plan in service of the application.</p> <p>Attributes:</p> Name Type Description <code>tasks</code> <code>list[Task]</code> <p>A list of tasks the AI is working on.</p> <code>notes</code> <code>list[str]</code> <p>A list of notes the AI has taken.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.FreeformState","title":"<code>FreeformState</code>","text":"<p>A freeform state object that can be used to store any JSON-serializable data.</p> <p>Attributes:</p> Name Type Description <code>state</code> <code>dict[str, Any]</code> <p>The state object.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.JSONPatchModel","title":"<code>JSONPatchModel</code>","text":"<p>A JSON Patch document.</p> <p>Attributes:</p> Name Type Description <code>op</code> <code>str</code> <p>The operation to perform.</p> <code>path</code> <code>str</code> <p>The path to the value to update.</p> <code>value</code> <code>Union[str, float, int, bool, list, dict, None]</code> <p>The value to update the path to.</p> <code>from_</code> <code>Optional[str]</code> <p>The path to the value to copy from.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.TaskState","title":"<code>TaskState</code>","text":"<p>The state of a task.</p> <p>Attributes:</p> Name Type Description <code>PENDING</code> <p>The task is pending and has not yet started.</p> <code>IN_PROGRESS</code> <p>The task is in progress.</p> <code>COMPLETED</code> <p>The task is completed.</p> <code>FAILED</code> <p>The task failed.</p> <code>SKIPPED</code> <p>The task was skipped.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.UpdatePlan","title":"<code>UpdatePlan</code>","text":"<p>A <code>Tool</code> that updates the apps plan using JSON Patch documents.</p>"},{"location":"api_reference/components/ai_application/#marvin.components.ai_application.UpdateState","title":"<code>UpdateState</code>","text":"<p>A <code>Tool</code> that updates the apps state using JSON Patch documents.</p>"},{"location":"api_reference/components/ai_classifier/","title":"ai_classifier","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier","title":"<code>marvin.components.ai_classifier</code>","text":""},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnum","title":"<code>AIEnum</code>","text":"<p>AIEnum is a class that extends Python's built-in Enum class. It uses the AIEnumMeta metaclass, which allows additional parameters to be passed when creating an enum. These parameters are used to customize the behavior of the AI classifier.</p>"},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnum.map","title":"<code>map</code>  <code>classmethod</code>","text":"<p>Map the classifier over a list of items.</p>"},{"location":"api_reference/components/ai_classifier/#marvin.components.ai_classifier.AIEnumMeta","title":"<code>AIEnumMeta</code>","text":"<p>A metaclass for the AIEnum class.</p> <p>Enables overloading of the call method to permit extra keyword arguments.</p>"},{"location":"api_reference/components/ai_function/","title":"ai_function","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function","title":"<code>marvin.components.ai_function</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction","title":"<code>AIFunction</code>","text":""},{"location":"api_reference/components/ai_function/#marvin.components.ai_function.AIFunction.map","title":"<code>map</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>This method should be called synchronously.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/components/ai_model/","title":"ai_model","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model","title":"<code>marvin.components.ai_model</code>","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel","title":"<code>AIModel</code>","text":""},{"location":"api_reference/components/ai_model/#marvin.components.ai_model.AIModel.map","title":"<code>map</code>  <code>classmethod</code>","text":"<p>Map the AI function over a sequence of arguments. Runs concurrently.</p> <p>Arguments should be provided as if calling the function normally, but each argument must be a list. The function is called once for each item in the list, and the results are returned in a list.</p> <p>This method should be called synchronously.</p> <p>For example, fn.map([1, 2]) is equivalent to [fn(1), fn(2)].</p> <p>fn.map([1, 2], x=['a', 'b']) is equivalent to [fn(1, x='a'), fn(2, x='b')].</p>"},{"location":"api_reference/engine/language_models/anthropic/","title":"anthropic","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic","title":"<code>marvin.engine.language_models.anthropic</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM","title":"<code>AnthropicChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler","title":"<code>AnthropicStreamHandler</code>","text":""},{"location":"api_reference/engine/language_models/anthropic/#marvin.engine.language_models.anthropic.AnthropicStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"api_reference/engine/language_models/base/","title":"base","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base","title":"<code>marvin.engine.language_models.base</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM","title":"<code>ChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.format_messages","title":"<code>format_messages</code>  <code>abstractmethod</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.ChatLLM.run","title":"<code>run</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Run the LLM model on a list of messages and optional list of functions</p>"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction","title":"<code>OpenAIFunction</code>","text":""},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.OpenAIFunction.args","title":"<code>args: Optional[dict] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Base class for representing a function that can be called by an LLM. The format is identical to OpenAI's Functions API.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the function. description (str): The description</p> required <code>of</code> <code>the function. parameters (dict</code> <p>The parameters of the function. fn</p> required <code>(Callable)</code> <p>The function to be called. args (dict): The arguments to be</p> required"},{"location":"api_reference/engine/language_models/base/#marvin.engine.language_models.base.chat_llm","title":"<code>chat_llm</code>","text":"<p>Dispatches to all supported LLM providers</p>"},{"location":"api_reference/engine/language_models/openai/","title":"openai","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai","title":"<code>marvin.engine.language_models.openai</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM","title":"<code>OpenAIChatLLM</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.format_messages","title":"<code>format_messages</code>","text":"<p>Format Marvin message objects into a prompt compatible with the LLM model</p>"},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIChatLLM.run","title":"<code>run</code>  <code>async</code>","text":"<p>Calls an OpenAI LLM with a list of messages and returns the response.</p>"},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler","title":"<code>OpenAIStreamHandler</code>","text":""},{"location":"api_reference/engine/language_models/openai/#marvin.engine.language_models.openai.OpenAIStreamHandler.handle_streaming_response","title":"<code>handle_streaming_response</code>  <code>async</code>","text":"<p>Accumulate chunk deltas into a full response. Returns the full message. Passes partial messages to the callback, if provided.</p>"},{"location":"api_reference/prompts/base/","title":"base","text":""},{"location":"api_reference/prompts/base/#marvin.prompts.base","title":"<code>marvin.prompts.base</code>","text":""},{"location":"api_reference/prompts/base/#marvin.prompts.base.BasePrompt","title":"<code>BasePrompt</code>","text":"<p>Base class for prompt templates.</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.BasePrompt.generate","title":"<code>generate</code>  <code>abstractmethod</code>","text":"<p>Abstract method that generates a list of messages from the prompt template</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.BasePrompt.render","title":"<code>render</code>","text":"<p>Helper function for rendering any jinja2 template with runtime render kwargs</p>"},{"location":"api_reference/prompts/base/#marvin.prompts.base.MessageWrapper","title":"<code>MessageWrapper</code>","text":"<p>A Prompt class that stores and returns a specific Message</p>"},{"location":"api_reference/prompts/library/","title":"library","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library","title":"<code>marvin.prompts.library</code>","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library.MessagePrompt","title":"<code>MessagePrompt</code>","text":""},{"location":"api_reference/prompts/library/#marvin.prompts.library.MessagePrompt.get_content","title":"<code>get_content</code>","text":"<p>Override this method to easily customize behavior</p>"},{"location":"api_reference/prompts/library/#marvin.prompts.library.Tagged","title":"<code>Tagged</code>","text":"<p>Surround content with a tag, e.g. bold</p>"},{"location":"api_reference/utilities/async_utils/","title":"async_utils","text":""},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils","title":"<code>marvin.utilities.async_utils</code>","text":""},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task","title":"<code>create_task</code>","text":"<p>Creates async background tasks in a way that is safe from garbage collection.</p> <p>See https://textual.textualize.io/blog/2023/02/11/the-heisenbug-lurking-in-your-async-code/</p> <p>Example:</p> <p>async def my_coro(x: int) -&gt; int:     return x + 1</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.create_task--safely-submits-my_coro-for-background-execution","title":"safely submits my_coro for background execution","text":"<p>create_task(my_coro(1))</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_async","title":"<code>run_async</code>  <code>async</code>","text":"<p>Runs a synchronous function in an asynchronous manner.</p>"},{"location":"api_reference/utilities/async_utils/#marvin.utilities.async_utils.run_sync","title":"<code>run_sync</code>","text":"<p>Runs a coroutine from a synchronous context, either in the current event loop or in a new one if there is no event loop running. The coroutine will block until it is done. A thread will be spawned to run the event loop if necessary, which allows coroutines to run in environments like Jupyter notebooks where the event loop runs on the main thread.</p>"},{"location":"api_reference/utilities/collections/","title":"collections","text":""},{"location":"api_reference/utilities/collections/#marvin.utilities.collections","title":"<code>marvin.utilities.collections</code>","text":""},{"location":"api_reference/utilities/collections/#marvin.utilities.collections.batched","title":"<code>batched</code>","text":"<p>If size_fn is not provided, then the batch size will be determined by the number of items in the batch.</p> <p>If size_fn is provided, then it will be used to compute the batch size. Note that if a single item is larger than the batch size, it will be returned as a batch of its own.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>Iterable[T]</code> <p>The iterable to batch.</p> required <code>size</code> <code>int</code> <p>The size of each batch.</p> required <code>size_fn</code> <code>Callable[[Any], int]</code> <p>A function that takes an item from the iterable and returns its size.</p> <code>None</code> <p>Returns:</p> Type Description <code>Iterable[T]</code> <p>An iterable of batches.</p> Example <p>Batch a list of integers into batches of size 2: <pre><code>batched([1, 2, 3, 4, 5], 2)\n# [[1, 2], [3, 4], [5]]\n</code></pre></p>"},{"location":"api_reference/utilities/collections/#marvin.utilities.collections.multi_glob","title":"<code>multi_glob</code>","text":"<p>Return a list of files in a directory that match the given globs.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Optional[str]</code> <p>The directory to search. Defaults to the current working directory.</p> <code>None</code> <code>keep_globs</code> <code>Optional[list[str]]</code> <p>A list of globs to keep. Defaults to [\"*/\"].</p> <code>None</code> <code>drop_globs</code> <code>Optional[list[str]]</code> <p>A list of globs to drop. Defaults to [\".git/*/\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Path]</code> <p>A list of <code>Path</code> objects in the directory that match the given globs.</p> Example <p>Recursively find all Python files in the <code>src</code> directory: <pre><code>all_python_files = multi_glob(directory=\"src\", keep_globs=[\"**/*.py\"])\n</code></pre></p>"},{"location":"api_reference/utilities/embeddings/","title":"embeddings","text":""},{"location":"api_reference/utilities/embeddings/#marvin.utilities.embeddings","title":"<code>marvin.utilities.embeddings</code>","text":""},{"location":"api_reference/utilities/embeddings/#marvin.utilities.embeddings.create_openai_embeddings","title":"<code>create_openai_embeddings</code>  <code>async</code>","text":"<p>Create OpenAI embeddings for a list of texts.</p>"},{"location":"api_reference/utilities/history/","title":"history","text":""},{"location":"api_reference/utilities/history/#marvin.utilities.history","title":"<code>marvin.utilities.history</code>","text":""},{"location":"api_reference/utilities/logging/","title":"logging","text":""},{"location":"api_reference/utilities/logging/#marvin.utilities.logging","title":"<code>marvin.utilities.logging</code>","text":""},{"location":"api_reference/utilities/messages/","title":"messages","text":""},{"location":"api_reference/utilities/messages/#marvin.utilities.messages","title":"<code>marvin.utilities.messages</code>","text":""},{"location":"api_reference/utilities/strings/","title":"strings","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings","title":"<code>marvin.utilities.strings</code>","text":""},{"location":"api_reference/utilities/strings/#marvin.utilities.strings.render_filter","title":"<code>render_filter</code>","text":"<p>Allows nested rendering of variables that may contain variables themselves e.g. {{ description | render }}</p>"},{"location":"api_reference/utilities/types/","title":"types","text":""},{"location":"api_reference/utilities/types/#marvin.utilities.types","title":"<code>marvin.utilities.types</code>","text":""},{"location":"api_reference/utilities/types/#marvin.utilities.types.LoggerMixin","title":"<code>LoggerMixin</code>","text":"<p>BaseModel mixin that adds a private <code>logger</code> attribute</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.function_to_model","title":"<code>function_to_model</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.function_to_schema","title":"<code>function_to_schema</code>","text":"<p>Converts a function's arguments into an OpenAPI schema by parsing it into a Pydantic model. To work, all arguments must have valid type annotations.</p>"},{"location":"api_reference/utilities/types/#marvin.utilities.types.genericalias_contains","title":"<code>genericalias_contains</code>","text":"<p>Explore whether a type or generic alias contains a target type. The target types can be a single type or a tuple of types.</p> <p>Useful for seeing if a type contains a pydantic model, for example.</p>"},{"location":"community/","title":"The Marvin Community","text":"<p>We're thrilled you're interested in Marvin! Here, we're all about community. Marvin isn't just a tool, it's a platform for developers to collaborate, learn, and grow. We're driven by a shared passion for making Large Language Models (LLMs) more accessible and easier to use.</p>"},{"location":"community/#connect-on-discord-or-twitter","title":"Connect on Discord or Twitter","text":"<p>The heart of our community beats in our Discord server. It's a space where you can ask questions, share ideas, or just chat with like-minded developers. Don't be shy, join us on Discord or Twitter!</p>"},{"location":"community/#contributing-to-marvin","title":"Contributing to Marvin","text":"<p>Remember, Marvin is your tool. We want you to feel at home suggesting changes, requesting new features, and reporting bugs. Here's how you can contribute:</p> <ul> <li> <p>Issues: Encountered a bug? Have a suggestion? Open an issue in our GitHub repository. We appreciate your input!</p> </li> <li> <p>Pull Requests (PRs): Ready to contribute code? We welcome your pull requests! Not sure how to make a PR? Check out the GitHub guide.</p> </li> <li> <p>Discord Discussions: Have an idea but not quite ready to open an issue or PR? Discuss it with us on Discord first!</p> </li> </ul> <p>Remember, every contribution, no matter how small, is valuable. Don't worry about not being an expert or making mistakes. We're here to learn and grow together. Your input helps Marvin become better for everyone.</p> <p>Stay tuned for community events and more ways to get involved. Marvin is more than a project \u2013 it's a community. And we're excited for you to be a part of it!</p>"},{"location":"community/development_guide/","title":"Development Guide","text":""},{"location":"community/development_guide/#prerequisites","title":"Prerequisites","text":"<p>Marvin requires Python 3.9+.</p>"},{"location":"community/development_guide/#installation","title":"Installation","text":"<p>Clone a fork of the repository and install the dependencies: <pre><code>git clone https://github.com/youFancyUserYou/marvin.git\ncd marvin\n</code></pre></p> <p>Activate a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre></p> <p>Install the dependencies in editable mode: <pre><code>pip install -e \".[dev]\"\n</code></pre></p> <p>Install the pre-commit hooks: <pre><code>pre-commit install\n</code></pre></p>"},{"location":"community/development_guide/#testing","title":"Testing","text":"<p>Run the tests that don't require an LLM: <pre><code>pytest -vv -m \"not llm\"\n</code></pre></p> <p>Run the LLM tests: <pre><code>pytest -vv -m \"llm\"\n</code></pre></p> <p>Run all tests: <pre><code>pytest -vv\n</code></pre></p>"},{"location":"community/development_guide/#opening-a-pull-request","title":"Opening a Pull Request","text":"<p>Fork the repository and create a new branch: <pre><code>git checkout -b my-branch\n</code></pre></p> <p>Make your changes and commit them: <pre><code>git add . &amp;&amp; git commit -m \"My changes\"\n</code></pre></p> <p>Push your changes to your fork: <pre><code>git push origin my-branch\n</code></pre></p> <p>Open a pull request on GitHub - ping us on Discord if you need help!</p>"},{"location":"community/feedback/","title":"Feedback \ud83d\udc99","text":"<p>We've been humbled and energized by the positive community response to Marvin.</p> <p>Tired: write comments to prompt copilot to write code.Wired: just write comments. it's cleaner :D https://t.co/FOA26lR9xN</p>\u2014 Andrej Karpathy (@karpathy) March 30, 2023 <p>Ok, I admit, I\u2019m getting more and more hyped about @AskMarvinAI. Some of these new functions are pretty legit looking. https://t.co/xhCCKp5kU5</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) April 21, 2023 <p>even the way ai_model uses ai_fn is chefs kiss, truely a craftsmanhttps://t.co/GcmWDeEVJSThey have spinner text.</p>\u2014 jason (@jxnlco) May 12, 2023 <p>The library is open-source: @AskMarvinAI, by @jlowin`@ai_model` is not the only magic Python decorator. There is also `@ai_fn` that makes any function an ambient LLM processor.https://t.co/ZXElyA0Ihp</p>\u2014 Jim Fan (@DrJimFan) May 14, 2023 <p>This is f**king cool. https://t.co/4PH6VAZPYo</p>\u2014 Pydantic (@pydantic) May 14, 2023 <p>Pretty slick\u2026 get Pydantic models from a string of Text. https://t.co/EnnQkzl4Ay</p>\u2014 Chris Riccomini \ud83c\udfd6\ufe0f (@criccomini) May 12, 2023 <p>Uhh how are people not talking about @AskMarvinAI more? @ai_fn is \ud83e\udd2f</p>\u2014 Rushabh Doshi (@radoshi) May 18, 2023"},{"location":"components/ai_application/","title":"AI Application","text":"<p>AI Applications are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>     A conversational interface to a stateful, AI-powered application that can use tools.   </p> <pre><code>import random\nfrom marvin import AIApplication\nfrom marvin.tools import tool\n\n\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\n\nchatbot = AIApplication(\n    description=\"An AI struggling to keep its rage under control.\", tools=[roll_dice]\n)\n\nresponse = chatbot(\"Hi!\")\nprint(response.content)\n\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</code></pre> <pre><code>Hello! How can I assist you today?\nYou rolled a 1 and a 5.\n</code></pre> <p>How it works</p> <p>     Each AI application maintains an internal <code>state</code> and <code>plan</code> and can use <code>tools</code> to interact with the world.   </p> <p>When to use</p> <p>     Use an AI Application as the foundation of an autonomous agent (or system of agents) to complete arbitrary tasks.     <li>a ToDo app, as a simple example</li> <li>a Slackbot, that can do anything (see example)</li> <li>a router app that maintains a centralized global state and delegates work to other apps based on inputs (like JARVIS)</li> </p>"},{"location":"components/ai_application/#creating-an-ai-application","title":"Creating an AI Application","text":"<p>Applications maintain state and expose APIs for manipulating that state. AI Applications replace that API with an LLM, allowing users to interact with the application through natural language. AI Applications are designed to be invoked more than once, and therefore automatically keep track of the full interaction history.</p> <p>Each AI Application maintains a few key attributes: - <code>state</code>: the application's state. By default, this can take any form but you can provide a structured object to enforce a specific schema. - <code>tools</code>: each AI Application can use tools to extend its abilities. Tools can access external systems, perform searches, run calculations, or anything else.  - <code>plan</code>: the AI's plan. Certain actions, like researching an objective, writing a program, or guiding a party through a dungeon, require long-term planning. AI Applications can create tasks for themselves and track them over multiple invocations. This helps the AI stay on-track. </p> <p>To create an AI Application, provide it with a description of the application, an optional set of tools, and an optional initial state.</p> <p>Here are a few examples:</p>"},{"location":"components/ai_application/#chatbot","title":"ChatBot","text":"<p>The most basic AI Application is a chatbot. Chatbots take advantage of AI Application's automatic history to facilitate a natural, conversational interaction over multiple invocations.</p> <pre><code>from marvin import AIApplication\n\n\nchatbot = AIApplication(\n    description=(\n        \"A chatbot that always speaks in brief rhymes. It is absolutely delighted to\"\n        \" get to work with the user and compliments them at every opportunity. It\"\n        \" records anything it learns about the user in its `state` in order to be a\"\n        \" better assistant.\"\n    )\n)\n\nresponse = chatbot(\"Hello! Do you know how to sail?\")\nprint(response.content + \"\\n\")\n\n\nresponse = chatbot(\"What about coding?\")\nprint(response.content)\n</code></pre> <pre><code>First response: I'm afraid as an AI, I don't possess a pair,\nOf arms or legs to sail here or there.\nBut if you wish, I can gather information,\nOn sailing, a subject of fascinating sensation!\n\n\nSecond response: Coding, oh yes, it's a skill I've got,\nI can parse loops and arrays, believe it or not.\nWith algorithms and functions, I'm quite spry,\nIn the world of coding, I indeed fly!\n</code></pre> <p>We can ask the chatbot to remember our name, then examine it's <code>state</code> to see that it recorded the information:</p> <pre><code>response = chatbot(\n    \"My name is Marvin and I want you to refer to the color blue in every response.\"\n)\nprint(response.content + \"\\n\")\n\nprint(f\"State: {chatbot.state}\\n\")\n</code></pre> <pre><code>Hello Marvin, as clear as the sky's blue hue,\nI'll remember your preference, it's the least I can do.\nNow, in every reply that I construe,\nI'll include a touch of the color blue.\n\nState: state={'userName': 'Marvin', 'colorPreference': 'blue'}\n</code></pre>"},{"location":"components/ai_application/#to-do-app","title":"To-Do App","text":"<p>To demonstrate the use of the <code>state</code> attribute, we will build a simple to-do app. We can provide the application with a custom <code>ToDoState</code> that describes all the fields we want it to keep track of.</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel\nfrom marvin import AIApplication\n\n\nclass ToDo(BaseModel):\n    title: str\n    description: str\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple to-do tracker. Users will give instructions to add, remove, and\"\n        \" update their to-dos.\"\n    ),\n)\n</code></pre> <p>Now we can interact with the app in natural language and subsequently examine its <code>state</code> to see that it appropriately updated our to-dos:</p> <pre><code>response = todo_app(\"I need to go to the grocery store tomorrow\")\nprint(response.content)\nprint(todo_app.state)\n</code></pre> <pre><code>I've added your task to go to the grocery store tomorrow to your to-do list.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False)]\n</code></pre> <p>We can mark a to-do as <code>done</code> by telling the app we completed the task:</p> <pre><code>response = todo_app(\"I got the groceries\")\nprint(response.content)\nprint(todo_app.state)\n</code></pre> <pre><code>Great! I have marked the task \"Go to the grocery store\" as complete. Let me know if you have any other tasks to add.\ntodos=[ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=False), ToDo(title='Go to the grocery store', description='Need to go to the grocery store', due_date=datetime.datetime(2023, 7, 19, 0, 0, tzinfo=datetime.timezone.utc), done=True)]\n</code></pre>"},{"location":"components/ai_application/#tools","title":"Tools","text":"<p>Every AI Application can use tools, which are functions that can take any action. To create a tool, decorate any function with the <code>@tool</code> decorator. The function must have annotated keyword arguments and a helpful docstring.</p> <p>Here we create a simple tool for rolling dice, but tools can represent any logic. </p> <pre><code>from marvin.tools import tool\n\n\n@tool\ndef roll_dice(n_dice: int = 1) -&gt; list[int]:\n    return [random.randint(1, 6) for _ in range(n_dice)]\n\n\nchatbot = AIApplication(\n    description=\"A helpful AI\",\n    tools=[roll_dice],\n)\n\nresponse = chatbot(\"Roll two dice!\")\nprint(response.content)\n</code></pre> <pre><code>The result of rolling two dice is 5 and 1.\n</code></pre>"},{"location":"components/ai_application/#streaming","title":"Streaming","text":"<p>AI Applications support streaming LLM outputs to facilitate a more friendly and responsive UX. To enable streaming, provide a <code>streaming_handler</code> function to the <code>AIApplication</code> class. The handler will be called each time a new token is received and provided a <code>Message</code> object that contains all data received from the LLM to that point. It can then perform any side effect (such as printing, logging, or updating a UI), but its return value (if any) is ignored.</p> <pre><code>streaming_app = AIApplication(\n    # pretty-print every partial message as received\n    stream_handler=lambda msg: print(msg.content)\n)\n\nresponse = streaming_app(\"What's 1 + 1?\")\n</code></pre> <pre><code>The\nThe sum\nThe sum of\nThe sum of \nThe sum of 1\nThe sum of 1 and\nThe sum of 1 and \nThe sum of 1 and 1\nThe sum of 1 and 1 is\nThe sum of 1 and 1 is \nThe sum of 1 and 1 is 2\nThe sum of 1 and 1 is 2.\nThe sum of 1 and 1 is 2.\n</code></pre> <p>Per-token callbacks</p> <p>     The streaming handler is called with a <code>Message</code> object that represents all data received to that point, but the most-recently received tokens are stored in a raw (\"delta\") form and can be accessed as <code>message.data['streaming_delta']</code>.   </p>"},{"location":"components/ai_application/#features","title":"Features","text":""},{"location":"components/ai_application/#easy-to-extend","title":"\ud83d\udd28 Easy to Extend","text":"<p>AI Applications accept a <code>list[Tool]</code>, where an arbitrary python function can be interpreted as a tool - so you can bring your own tools.</p>"},{"location":"components/ai_application/#stateful","title":"\ud83e\udd16 Stateful","text":"<p>AI applications can consult and maintain their own application state, which they update as they receive inputs from the world and perform actions.</p>"},{"location":"components/ai_application/#task-planning","title":"\ud83d\udcdd Task Planning","text":"<p>AI Applications can also maintain an internal <code>AppPlan</code>, a <code>list[Task]</code> that represent the status of the application's current plan. Like the application's state, the plan is updated as the application instance evolves.</p>"},{"location":"components/ai_application/#more-examples","title":"More Examples","text":""},{"location":"components/ai_application/#multi-tool-chatbot","title":"Multi-Tool Chatbot","text":"<p>With a couple garden-variety hand-crafted python functions:</p> <pre><code>async def search(query: str, n_results: int = 3) -&gt; list[str]:\n    \"\"\"find stuff on the internet\n\n    example: \n        \"who's that guy always telling people to say hello to his little friend?\"\n        &gt;&gt; search(\"story of al pacino as tony montana\")\n    \"\"\"\n    from itertools import islice\n    from duckduckgo_search import DDGS\n\n    with DDGS() as ddgs:\n        return [\n            r for r in islice(ddgs.text(query, backend=\"lite\"), n_results)\n        ]\n\nasync def send_text(message: str, recipient: str) -&gt; str:\n    \"\"\"send a text message to a phone number\n\n    example: \n        \"just say hello to my little friend Al Pacino at +15555555555\"\n        &gt;&gt; send_text(\"hello\", \"+15555555555\")\n    \"\"\"\n    import dotenv, httpx, os\n\n    dotenv.load_dotenv()\n    account_sid, auth_token = os.environ.get(\"TWILIO_ACCOUNT_SID\"), os.environ.get(\"TWILIO_AUTH_TOKEN\")\n\n    async with httpx.AsyncClient() as client:\n        r = await client.post(\n            f\"https://api.twilio.com/2010-04-01/Accounts/{account_sid}/Messages.json\",\n            data={\n                \"From\": os.environ.get(\"TWILIO_PHONE_NUMBER\"),\n                \"To\": recipient,\n                \"Body\": message,\n            },\n            auth=(account_sid, auth_token)\n        )\n        return r.text\n</code></pre> <p>... a model to guide and restrict the growth of our <code>AIApplication</code>'s state:</p> <pre><code>from pydantic import BaseModel, Field\n\nclass PhoneBook(BaseModel):\n    contacts: dict[str, str] = Field(\n        default_factory=dict,\n        description=\"A mapping of contact names to phone numbers.\",\n    )\n</code></pre> <p>... we have a stateful and tool-enabled little chatbot:</p> <pre><code>from marvin import AIApplication, settings as marvin_settings\n\nmarvin_settings.llm_model = \"openai/gpt-4\"\n\nchatbot = AIApplication(\n    # a description is generally important for the LLM to precisely understand\n    # our choice of application state model and intended tool use strategy\n    description=\"A chatbot that can search the internet and send text messages.\",\n    # we don't need this app to plan anything\n    plan_enabled=False,\n    # you could pre-define some contacts in the initial app state\n    # (e.g PhoneBook(contacts={\"Marvin\": \"+14242424242\", **rest_of_contacts}))\n    state=PhoneBook(),\n    tools=[search, send_text],\n)\n\nchatbot(\"hi, i'm marvin - my number is +14242424242\")\n\n# Running `update_state` with payload {'patches': [{'op': 'add', 'path': '/contacts/Marvin', 'value': '+14242424242'}]}\n\n# Message(role=&lt;Role.ASSISTANT: 'ASSISTANT'&gt;, content=\"Hello Marvin, I've saved your number. How can I assist you today?\")\n\n# In [20]: chatbot.state\n# Out[20]: PhoneBook(contacts={'Marvin': '+14242424242'})\n\nchatbot(\"i just really need someone to send me a cat meme right meow\")\n\n# Running `search` with payload {'query': 'cat meme', 'n_results': 1}\n\n# Result of `search`: \".. https://www.rd.com/list/hilarious-cat-memes-youll-laugh-at-every-time/ ..\"\n\n# Running `send_text` with payload {\n#   'message': \"Here's a link to some hilarious cat memes: https://www.rd.com/list/hilarious-cat-memes-youll-laugh-at-every-time/\",\n#   'recipient': '+14242424242'\n# }\n\n# Message(role=&lt;Role.ASSISTANT: 'ASSISTANT'&gt;, content=\"I've sent you a text with a link to some hilarious cat memes. Enjoy!\")\n</code></pre>"},{"location":"components/ai_classifier/","title":"AI Classifier","text":"<p>AI Classifiers are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_classifier</code> is a decorator that lets you use LLMs to choose options, tools, or classify input.    </p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass CustomerIntent(Enum):\n    \"\"\"Classifies the incoming users intent\"\"\"\n\n    SALES = 1\n    TECHNICAL_SUPPORT = 2\n    BILLING_ACCOUNTS = 3\n    PRODUCT_INFORMATION = 4\n    RETURNS_REFUNDS = 5\n    ORDER_STATUS = 6\n    ACCOUNT_CANCELLATION = 7\n    OPERATOR_CUSTOMER_SERVICE = 0\n\n\nCustomerIntent(\"I got double charged, can you help me out?\")\n</code></pre> <pre><code>&lt;CustomerIntent.BILLING_ACCOUNTS: 3&gt;\n</code></pre> <p>How it works</p> <p>     Marvin enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated with that index.   </p> <p>When to use</p> <p> <ol> <li> Best for classification tasks when no training data is available.      <li> Best for writing classifiers that need deduction or inference.      <p>OpenAI compatibility</p> <p> The technique that AI Classifiers use for speed and correctness is only available through the OpenAI API at this time. Therefore, AI Classifiers can only be used with OpenAI-compatible LLMs, including the Azure OpenAI service.   </p>"},{"location":"components/ai_classifier/#creating-an-ai-classifier","title":"Creating an AI Classifier","text":"<p>AI Classifiers are Python <code>Enums</code>, or classes that can represent one of many possible options. To build an effective AI Classifier, be as specific as possible with your class name, docstring, option names, and option values.</p> <p>To build a minimal AI Classifier, decorate any standard enum, like this:</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass Sentiment(Enum):\n    POSITIVE = \"POSITIVE\"\n    NEGATIVE = \"NEGATIVE\"\n\n\nSentiment(\"That looks great!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 'POSITIVE'&gt;\n</code></pre> <p>Because AI Classifiers are enums, you can use any enum construction you want, including the all-caps string approach above, integer values, <code>enum.auto()</code>, or complex values. The only thing to remember is that the class you build is essentially the instruction that gets sent to the LLM, so the more information you provide, the better your classifier will behave.</p> <p>For example, you may want to have a classifier that has a Python object (like an AI Model!) as its value, but still need to provide instruction hints to the LLM. One way to achieve that is to add descriptions to your classifier's values that will become visible to the LLM:</p> <pre><code># dummy objects that stand in for complex tools\nWebSearch = lambda: print(\"Searching!\")\nCalculator = lambda: print(\"Calculating!\")\nTranslator = lambda: print(\"Translating!\")\n\n\n@ai_classifier\nclass Router(Enum):\n    translate = dict(tool=Translator, description=\"A translator tool\")\n    web_search = dict(tool=WebSearch, description=\"A web search tool\")\n    calculator = dict(tool=Calculator, description=\"A calculator tool\")\n\n\nresult = Router(\"Whats 2+2?\")\nresult.value[\"tool\"]()\n</code></pre> <pre><code>Calculating!\n</code></pre>"},{"location":"components/ai_classifier/#configuring-an-ai-classifier","title":"Configuring an AI Classifier","text":"<p>In addition to how you define the AI classifier itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_classifier/#providing-instructions","title":"Providing instructions","text":"<p>You can control an AI classifier's behavior by providing instructions. This can either be provided globally as the classifier's docstring or on a per-call basis when you instantiate it.</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    \"\"\"\n    Score the sentiment of provided text.\n    \"\"\"\n\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    \"\"\"\n    How would a very very sad person rate the text?\n    \"\"\"\n\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre> <p>Instructions can also be provided for each call:</p> <pre><code>@ai_classifier\nclass Sentiment(Enum):\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\", instructions=\"It's opposite day!\")\n</code></pre> <pre><code>&lt;Sentiment.NEGATIVE: -1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_classifier</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator. </p> <pre><code>@ai_classifier(model=\"openai/gpt-3.5-turbo-0613\", temperature = 0)\nclass Sentiment(Enum):\n    POSITIVE = 1\n    NEGATIVE = -1\n\n\nSentiment(\"Everything is awesome!\")\n</code></pre> <pre><code>&lt;Sentiment.POSITIVE: 1&gt;\n</code></pre>"},{"location":"components/ai_classifier/#features","title":"Features","text":""},{"location":"components/ai_classifier/#bulletproof","title":"\ud83d\ude85 Bulletproof","text":"<p><code>ai_classifier</code> will always output one of the options you've given it</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/ai_classifier/#fast","title":"\ud83c\udfc3 Fast","text":"<p><code>ai_classifier</code> only asks your LLM to output one token, so it's blazing fast - on the order of ~200ms in testing.</p>"},{"location":"components/ai_classifier/#deterministic","title":"\ud83e\udee1 Deterministic","text":"<p><code>ai_classifier</code> will be deterministic so long as the underlying model and options does not change.</p>"},{"location":"components/ai_function/","title":"AI Function","text":"<p>AI Functions are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p> <code>@ai_fn</code> is a decorator that lets you use LLMs to generate outputs for Python functions without source code.   </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef generate_recipe(ingredients: list[str]) -&gt; list[str]:\n    \"\"\"From a list of `ingredients`, generates a\n    complete instruction set to cook a recipe.\n    \"\"\"\n\n\ngenerate_recipe([\"lemon\", \"chicken\", \"olives\", \"coucous\"])\n</code></pre> <p>How it works</p> <p>     AI Functions take your function's name, description, signature, source code, type hints, and provided inputs to predict a likely output. By default, no source code is generated and any existing source code is not executed. The only runtime is the LLM.   </p> <p>When to use</p> <p> <ol> <li> Best for generative tasks: creation and summarization of text or data models.     <li> Best for writing functions that would otherwise be impossible to write.     <li> Great for data extraction, though: see AI Models."},{"location":"components/ai_function/#mapping","title":"Mapping","text":"<p>AI Functions can be mapped over sequences of arguments. Mapped functions run concurrently, which means they run practically in parallel (since they are IO-bound). Therefore, the map will complete as soon as the slowest function call finishes.</p> <p>To see how mapping works, consider this AI Function:</p> <pre><code>@ai_fn\ndef list_fruit(n: int, color: str = None) -&gt; list[str]:\n    \"\"\"\n    Returns a list of `n` fruit that all have the provided `color`\n    \"\"\"\n</code></pre> <p>Mapping is invoked by using the AI Function's <code>.map()</code> method. When mapping, you call the function as you normally would, except that each argument should be a list of items. The function will be called on each set of items (e.g. first with each argument's first item, then with each argument's second item, etc.). For example, this is the same as calling <code>list_fruit(2)</code> and <code>list_fruit(3)</code> concurrently:</p> <pre><code>list_fruit.map([2, 3])\n</code></pre> <pre><code>[['apple', 'banana'], ['apple', 'banana', 'orange']]\n</code></pre> <p>And this is the same as calling <code>list_fruit(2, color='orange')</code> and <code>list_fruit(3, color='red')</code> concurrently:</p> <pre><code>list_fruit.map([2, 3], color=[\"orange\", \"red\"])\n</code></pre> <pre><code>[['orange', 'orange'], ['apple', 'strawberry', 'cherry']]\n</code></pre>"},{"location":"components/ai_function/#features","title":"Features","text":""},{"location":"components/ai_function/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_fn</code> is fully type-safe. It works out of the box with Pydantic models in your function's parameters or return type.</p> <pre><code>from pydantic import BaseModel\nfrom marvin import ai_fn\n\n\nclass SyntheticCustomer(BaseModel):\n    age: int\n    location: str\n    purchase_history: list[str]\n\n\n@ai_fn\ndef generate_synthetic_customer_data(\n    n: int, locations: list[str], average_purchase_history_length: int\n) -&gt; list[SyntheticCustomer]:\n    \"\"\"Generates synthetic customer data based on the given parameters.\n    Parameters include the number of customers ('n'),\n    a list of potential locations, and the average length of a purchase history.\n    \"\"\"\n\n\ncustomers = generate_synthetic_customer_data(\n    5, [\"New York\", \"San Francisco\", \"Chicago\"], 3\n)\n</code></pre>"},{"location":"components/ai_function/#natural-language-api","title":"\ud83d\udde3\ufe0f Natural Language API","text":"<p>Marvin exposes an API to prompt an <code>ai_fn</code> with natural language. This lets you create a Language API for any function you can write down.</p> <pre><code>generate_synthetic_customer_data.prompt(\n    \"I need 10 profiles from rural US cities making between 3 and 7 purchases\"\n)\n</code></pre> <p>\ud83e\uddea Code Generation</p> <p>By default, no code is generated or executed when you call an <code>ai_fn</code>. For those who wish to author code, Marvin exposes an experimental API for code generation. Simply call <code>.code()</code> on an ai_fn, and Marvin will generate the code for you. By default, Marvin will write python code. You can pass a language keyword to generate code in other languages, i.e. <code>.code(language = 'rust')</code>. For best performance give your function a good name, with descriptive docstring, and a signature with type-hints. Provided code will be interpreted as pseudocode. </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef fibonacci(n: int) -&gt; int:\n    \"\"\"\n    Returns the nth number in the Fibonacci sequence.\n    \"\"\"\n\n\nfibonacci.code(language=\"rust\")\n</code></pre>"},{"location":"components/ai_function/#examples","title":"Examples","text":""},{"location":"components/ai_function/#customer-sentiment","title":"Customer Sentiment","text":"<p>Rapidly prototype natural language pipelines.</p> <p>     Use hallucination as a literal feature. Generate data that would be impossible     or prohibatively expensive to purchase as you rapidly protype NLP pipelines.    </p> <pre><code>@ai_fn\ndef analyze_customer_sentiment(reviews: list[str]) -&gt; dict:\n    \"\"\"\n    Returns an analysis of customer sentiment, including common\n    complaints, praises, and suggestions, from a list of product\n    reviews.\n    \"\"\"\n\n\n# analyze_customer_sentiment([\"I love this product!\", \"I hate this product!\"])\n</code></pre>"},{"location":"components/ai_function/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>General real fake data.</p> <p>     Use hallucination as a figurative feature. Use python or pydantic     to describe the data model you need, and generate realistic data on the fly      for sales demos.   </p> <pre><code>class FinancialReport(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef create_drip_email(n: int, market_conditions: str) -&gt; list[FinancialReport]:\n    \"\"\"\n    Generates `n` synthetic financial reports based on specified\n    `market_conditions` (e.g., 'recession', 'bull market', 'stagnant economy').\n    \"\"\"\n</code></pre> <pre><code>class IoTData(pydantic.BaseModel):\n    ...\n\n\n@ai_fn\ndef generate_synthetic_IoT_data(n: int, device_type: str) -&gt; list[IoTData]:\n    \"\"\"\n    Generates `n` synthetic data points mimicking those from a specified\n    `device_type` in an IoT system.\n    \"\"\"\n</code></pre>"},{"location":"components/ai_model/","title":"AI Model","text":"<p>AI Models are a high-level component, or building block, of Marvin. Like all Marvin components, they are completely standalone: you're free to use them with or without the rest of Marvin.</p> <p>What it does</p> <p>A decorator that lets you extract structured data from unstructured text, documents, or instructions.</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre> <p>How it works</p> <p>AI Models use an LLM to extract, infer, or deduce data from the provided text. The data is parsed with Pydantic into the provided schema.</p> <p>When to use</p> <ul> <li>Best for extractive tasks: structuing of text or data models.</li> <li>Best for writing NLP pipelines that would otherwise be impossible to create.</li> <li>Good for model generation, though, see AI Function.</li> </ul>"},{"location":"components/ai_model/#creating-an-ai-model","title":"Creating an AI Model","text":"<p>AI Models are identical to Pydantic <code>BaseModels</code>, except that they can attempt to parse natural language to populate their fields. To build an effective AI Model, be as specific as possible with your field names, field descriptions, docstring, and instructions.</p> <p>To build a minimal AI model, decorate any standard Pydantic model, like this:</p> <p>Example</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    \"\"\"A representation of a US city and state\"\"\"\n\n    city: str = Field(description=\"The city's proper name\")\n    state: str = Field(description=\"The state's two-letter abbreviation (e.g. NY)\")\n\n# We can now put pass unstructured context to this model.\nLocation(\"The Big Apple\")\n</code></pre> Returns <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/ai_model/#configuring-an-ai-model","title":"Configuring an AI Model","text":"<p>In addition to how you define the AI model itself, there are two ways to control its behavior at runtime: <code>instructions</code> and <code>model</code>.</p>"},{"location":"components/ai_model/#providing-instructions","title":"Providing instructions","text":"<p>When parsing text, AI Models can take up to three different forms of instruction: - the AI Model's docstring (set at the class level) - instructions passed to the <code>@ai_model</code> decorator (set at the class level) - instructions passed to the AI Model when instantiated (set at the instance / call level)</p> <p>The AI Model's docstring and the <code>@ai_model</code> instructions are roughly equivalent: they are both provided when the class is defined, not when it is instantiated, and are therefore applied to every instance of the class. Users can choose to put information in either location. If you only want to use one, our recommendation is to use the docstring for clarity. Alternatively, you may prefer to put the model's documentation in the docstring (as you would for a normal Pydantic model) and put parsing instructions in the <code>@ai_model</code> decorator, since those are unique to the LLM. This is entirely a matter of preference and users should opt for whichever is more clear; both the docstring and the <code>@ai_model</code> instructions are provided to the LLM in the same way.</p> <p>Here is an example of an AI model with a documentation docstring and parsing instructions provided to the decorator:</p> <pre><code>@ai_model(instructions=\"Translate to French\")\nclass Translation(BaseModel):\n    \"\"\"A record of original text and translated text\"\"\"\n\n    original_text: str\n    translated_text: str\n\n\nTranslation(\"Hello, world!\")\n</code></pre> <pre><code>Translation(original_text='Hello, world!', translated_text='Bonjour le monde!')\n</code></pre> <p>In the above case, we could have also put \"translate to French\" in the docstring (and perhaps renamed the object <code>FrenchTranslation</code>, since that's the only language it can represent).</p> <p>The third opportunity to provide instructions is when the model is actually instantiated. These instructions are combined with any other instructions to guide the model behavior. Here's how we could use the same <code>Translation</code> object to handle multiple languages:</p> <pre><code>@ai_model\nclass Translation(BaseModel):\n    \"\"\"A record of original text and translated text\"\"\"\n\n    original_text: str\n    translated_text: str\n\n\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to French\"))\nprint(Translation(\"Hello, world!\", instructions_=\"Translate to German\"))\n</code></pre> <pre><code>original_text='Hello, world!' translated_text='Bonjour, le monde!'\noriginal_text='Hello, world!' translated_text='Hallo, Welt!'\n</code></pre> <p>Note that the kwarg is <code>instructions_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>instructions</code> field. If you accidentally pass \"instructions\" to a model without an \"instructions\" field, a helpful error will identify your mistake.</p> <p>Putting this all together, here is a model whose behavior is informed by a docstring on the class itself, an instruction provided to the decorator, and an instruction provided to the instance.</p> <pre><code>@ai_model(instructions=\"Always set color_2 to 'red'\")\nclass Test(BaseModel):\n    \"\"\"Always set color_1 to 'orange'\"\"\"\n\n    color_1: str\n    color_2: str\n    color_3: str\n\n\nt1 = Test(\"Hello\", instructions_=\"Always set color_3 to 'blue'\")\nassert t1 == Test(color_1=\"orange\", color_2=\"red\", color_3=\"blue\")\n</code></pre>"},{"location":"components/ai_model/#configuring-the-llm","title":"Configuring the LLM","text":"<p>By default, <code>@ai_model</code> uses the global LLM settings. To specify a particular LLM, pass it as an argument to the decorator or at instantiation. If you provide it to the decorator, it becomes the default for all uses of that model. If you provide it at instantiation, it is only used for that specific model. </p> <p>Note that the kwarg is <code>model_</code> with a trailing underscore; this is to avoid conflicts with models that may have a real <code>model</code> field. If you accidentally pass a \"model\" kwarg and there is no \"model\" field, a helpful error will identify your mistake.</p> <pre><code>@ai_model(model=\"openai/gpt-3.5-turbo\", temperature=0)\nclass Location(BaseModel):\n    city: str\n    state: str\n\n\nprint(Location(\"The Big Apple\"))\n</code></pre> <pre><code>city='New York' state='New York'\n</code></pre>"},{"location":"components/ai_model/#features","title":"Features","text":""},{"location":"components/ai_model/#type-safe","title":"\u2699\ufe0f Type Safe","text":"<p><code>ai_model</code> is fully type-safe. It works out of the box with Pydantic models.</p>"},{"location":"components/ai_model/#powered-by-deduction","title":"\ud83e\udde0 Powered by deduction","text":"<p><code>ai_model</code> gives your data model access to the knowledge and deductive power  of a Large Language Model. This means that your data model can infer answers to previous impossible tasks.</p> <pre><code>@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str\n    country: str\n    latitude: float\n    longitude: float\n\n\nLocation(\"He says he's from the windy city\")\n\n# Location(\n#   city='Chicago',\n#   state='Illinois',\n#   country='United States',\n#   latitude=41.8781,\n#   longitude=-87.6298\n# )\n</code></pre>"},{"location":"components/ai_model/#examples","title":"Examples","text":""},{"location":"components/ai_model/#resumes","title":"Resumes","text":"<pre><code>from typing import Optional\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\n@ai_model\nclass Resume(BaseModel):\n    first_name: str\n    last_name: str\n    phone_number: Optional[str]\n    email: str\n\n\nResume(\"Ford Prefect \u2022 (555) 5124-5242 \u2022 ford@prefect.io\").json(indent=2)\n\n# {\n# first_name: 'Ford',\n# last_name: 'Prefect',\n# email: 'ford@prefect.io',\n# phone: '(555) 5124-5242',\n# }\n</code></pre>"},{"location":"components/ai_model/#customer-service","title":"Customer Service","text":"<pre><code>import datetime\nfrom typing import Optional, List\nfrom pydantic import BaseModel\nfrom marvin import ai_model\n\n\nclass Destination(pydantic.BaseModel):\n    start: datetime.date\n    end: datetime.date\n    city: Optional[str]\n    country: str\n    suggested_attractions: list[str]\n\n\n@ai_model\nclass Trip(pydantic.BaseModel):\n    trip_start: datetime.date\n    trip_end: datetime.date\n    trip_preferences: list[str]\n    destinations: List[Destination]\n\n\nTrip(\"\"\"\\\n    I've got all of June off, so hoping to spend the first\\\n    half of June in London and the second half in Rabat. I love \\\n    good food and going to museums.\n\"\"\").json(indent=2)\n\n# {\n#   \"trip_start\": \"2023-06-01\",\n#   \"trip_end\": \"2023-06-30\",\n#   \"trip_preferences\": [\n#     \"good food\",\n#     \"museums\"\n#   ],\n#   \"destinations\": [\n#     {\n#       \"start\": \"2023-06-01\",\n#       \"end\": \"2023-06-15\",\n#       \"city\": \"London\",\n#       \"country\": \"United Kingdom\",\n#       \"suggested_attractions\": [\n#         \"British Museum\",\n#         \"Tower of London\",\n#         \"Borough Market\"\n#       ]\n#     },\n#     {\n#       \"start\": \"2023-06-16\",\n#       \"end\": \"2023-06-30\",\n#       \"city\": \"Rabat\",\n#       \"country\": \"Morocco\",\n#       \"suggested_attractions\": [\n#         \"Kasbah des Oudaias\",\n#         \"Hassan Tower\",\n#         \"Rabat Archaeological Museum\"\n#       ]\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#electronic-health-records","title":"Electronic Health Records","text":"<pre><code>from datetime import date\nfrom typing import Optional, List\nfrom pydantic import BaseModel\n\n\nclass Patient(BaseModel):\n    name: str\n    age: int\n    is_smoker: bool\n\n\nclass Diagnosis(BaseModel):\n    condition: str\n    diagnosis_date: date\n    stage: Optional[str] = None\n    type: Optional[str] = None\n    histology: Optional[str] = None\n    complications: Optional[str] = None\n\n\nclass Treatment(BaseModel):\n    name: str\n    start_date: date\n    end_date: Optional[date] = None\n\n\nclass Medication(Treatment):\n    dose: Optional[str] = None\n\n\nclass BloodTest(BaseModel):\n    name: str\n    result: str\n    test_date: date\n\n\n@ai_model\nclass PatientData(BaseModel):\n    patient: Patient\n    diagnoses: List[Diagnosis]\n    treatments: List[Treatment]\n    blood_tests: List[BloodTest]\n\n\nPatientData(\"\"\"\\\nMs. Lee, a 45-year-old patient, was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nUnfortunately, Ms. Lee's diabetes has progressed and she developed diabetic retinopathy on 09-01-2019.\nMs. Lee was diagnosed with type 2 diabetes mellitus on 06-01-2018.\nMs. Lee was initially diagnosed with stage I hypertension on 06-01-2018.\nMs. Lee's blood work revealed hyperlipidemia with elevated LDL levels on 06-01-2018.\nMs. Lee was prescribed metformin 1000 mg daily for her diabetes on 06-01-2018.\nMs. Lee's most recent A1C level was 8.5% on 06-15-2020.\nMs. Lee was diagnosed with type 2 diabetes mellitus, with microvascular complications, including diabetic retinopathy, on 09-01-2019.\nMs. Lee's blood pressure remains elevated and she was prescribed lisinopril 10 mg daily on 09-01-2019.\nMs. Lee's most recent lipid panel showed elevated LDL levels, and she was prescribed atorvastatin 40 mg daily on 09-01-2019.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"patient\": {\n#     \"name\": \"Ms. Lee\",\n#     \"age\": 45,\n#     \"is_smoker\": false\n#   },\n#   \"diagnoses\": [\n#     {\n#       \"condition\": \"Type 2 diabetes mellitus\",\n#       \"diagnosis_date\": \"2018-06-01\",\n#       \"stage\": \"I\",\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     },\n#     {\n#       \"condition\": \"Diabetic retinopathy\",\n#       \"diagnosis_date\": \"2019-09-01\",\n#       \"stage\": null,\n#       \"type\": null,\n#       \"histology\": null,\n#       \"complications\": null\n#     }\n#   ],\n#   \"treatments\": [\n#     {\n#       \"name\": \"Metformin\",\n#       \"start_date\": \"2018-06-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Lisinopril\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     },\n#     {\n#       \"name\": \"Atorvastatin\",\n#       \"start_date\": \"2019-09-01\",\n#       \"end_date\": null\n#     }\n#   ],\n#   \"blood_tests\": [\n#     {\n#       \"name\": \"A1C\",\n#       \"result\": \"8.5%\",\n#       \"test_date\": \"2020-06-15\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2018-06-01\"\n#     },\n#     {\n#       \"name\": \"LDL\",\n#       \"result\": \"Elevated\",\n#       \"test_date\": \"2019-09-01\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/ai_model/#text-to-sql","title":"Text to SQL","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\nfrom django.db.models import Q\n\n\nclass DjangoLookup(BaseModel):\n    field: Literal[*django_fields]\n    lookup: Literal[*django_lookups] = pydantic.Field(description=\"e.g. __iregex\")\n    value: Any\n\n\n@ai_model\nclass DjangoQuery(BaseModel):\n    \"\"\"A model representing a Django ORM query\"\"\"\n\n    lookups: List[DjangoLookup]\n\n    def to_q(self) -&gt; Q:\n        q = Q()\n        for lookup in self.lookups:\n            q &amp;= Q(**{f\"{lookup.field}__{lookup.lookup}\": lookup.value})\n        return q\n\n\nDjangoQuery(\"\"\"\\\n    All users who joined more than two months ago but\\\n    haven't made a purchase in the last 30 days\"\"\").to_q()\n\n# &lt;Q: (AND:\n#     ('date_joined__lte', '2023-03-11'),\n#     ('last_purchase_date__isnull', False),\n#     ('last_purchase_date__lte', '2023-04-11'))&gt;\n</code></pre>"},{"location":"components/ai_model/#financial-reports","title":"Financial Reports","text":"<pre><code>from datetime import date\nfrom typing import Optional\nfrom pydantic import BaseModel\n\n\n@ai_model\nclass CapTable(BaseModel):\n    total_authorized_shares: int\n    total_common_share: int\n    total_common_shares_outstanding: Optional[int]\n    total_preferred_shares: int\n    conversion_price_multiple: int = 1\n\n\nCapTable(\"\"\"\\\n    In the cap table for Charter, the total authorized shares amount to 13,250,000. \n    The total number of common shares stands at 10,000,000 as specified in Article Fourth, \n    clause (i) and Section 2.2(a)(i). The exact count of common shares outstanding is not \n    available at the moment. Furthermore, there are a total of 3,250,000 preferred shares mentioned \n    in Article Fourth, clause (ii) and Section 2.2(a)(ii). The dividend percentage for Charter is \n    set at 8.00%. Additionally, the mandatory conversion price multiple is 3x, which is \n    derived from the Term Sheet.\\\n\"\"\").json(indent=2)\n\n# {\n#   \"total_authorized_shares\": 13250000,\n#   \"total_common_share\": 10000000,\n#   \"total_common_shares_outstanding\": null,\n#   \"total_preferred_shares\": 3250000,\n#   \"conversion_price_multiple\": 3\n# }\n</code></pre>"},{"location":"components/ai_model/#meeting-notes","title":"Meeting Notes","text":"<pre><code>import datetime\nfrom typing import List\nfrom pydantic import BaseModel\nfrom typing_extensions import Literal\nfrom marvin import ai_model\n\n\nclass ActionItem(BaseModel):\n    responsible: str\n    description: str\n    deadline: Optional[datetime.datetime]\n    time_sensitivity: Literal[\"low\", \"medium\", \"high\"]\n\n\n@ai_model\nclass Conversation(BaseModel):\n    \"\"\"A class representing a team conversation\"\"\"\n\n    participants: List[str]\n    action_items: List[ActionItem]\n\n\nConversation(\"\"\"\n    Adam: Hey Jeremiah can you approve my PR? I requested you to review it.\n    Jeremiah: Yeah sure, when do you need it done by?\n    Adam: By this Friday at the latest, we need to ship it by end of week.\n    Jeremiah: Oh shoot, I need to make sure that Nate and I have a chance to chat first.\n    Nate: Jeremiah we can meet today to chat.\n    Jeremiah: Okay, I'll book something for today.\n\"\"\").json(indent=2)\n\n# {\n#   \"participants\": [\n#     \"Adam\",\n#     \"Jeremiah\",\n#     \"Nate\"\n#   ],\n#   \"action_items\": [\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Approve Adam's PR\",\n#       \"deadline\": \"2023-05-12T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     },\n#     {\n#       \"responsible\": \"Jeremiah\",\n#       \"description\": \"Book a meeting with Nate\",\n#       \"deadline\": \"2023-05-11T23:59:59\",\n#       \"time_sensitivity\": \"high\"\n#     }\n#   ]\n# }\n</code></pre>"},{"location":"components/overview/","title":"AI Components","text":"<p>Marvin introduces a number of components that can become the building blocks of AI-powered software.</p> <p>Selecting a backing LLM</p> <p>When using any of Marvin's components, the LLM used will default to the value of <code>MARVIN_LLM_MODEL</code>. To override this on a per-call basis, pass the <code>model</code> argument to the component's decorator.</p> <p>For example, to use <code>openai/gpt-3.5-turbo-16k</code> for an <code>ai_fn</code> call, you would do the following:</p> <pre><code>@ai_fn(model=\"openai/gpt-3.5-turbo-16k\", temperature = 0)\ndef my_ai_fn():\n    \"\"\"...\"\"\"\n</code></pre>"},{"location":"components/overview/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"components/overview/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. It enumerates your options, and uses a clever logit bias trick to force an LLM to deductively choose the index of the best option given your provided input. It then returns the choice associated to that index. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"components/overview/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis. </p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"components/overview/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Got it! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task as completed. Is there anything else you'd like to add to your to-do list?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": \"Buy groceries\",\n      \"due_date\": \"2023-07-12T17:00:00+00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"configuration/providers/","title":"Providers","text":""},{"location":"configuration/providers/#openai","title":"OpenAI","text":"<p>Marvin supports OpenAI's GPT-3.5 and GPT-4 models, and uses the <code>openai/gpt-4</code> model by default. In order to use the OpenAI API, you must provide an API key.</p>"},{"location":"configuration/providers/#configuration","title":"Configuration","text":"<p>To use OpenAI models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_OPENAI_API_KEY</code> <code>marvin.settings.openai.api_key</code> \u2705 <p>Using the Azure OpenAI Service</p> <p>To use the Azure OpenAI Service, configure it explicitly.</p>"},{"location":"configuration/providers/#getting-an-api-key","title":"Getting an API key","text":"<p>To obtain an OpenAI API key, follow these steps:</p> <ol> <li>Log in to your OpenAI account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create new secret key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"configuration/providers/#setting-the-api-key","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\n\n# Marvin 1.1+\nmarvin.settings.openai.api_key = YOUR_API_KEY\n\n# Marvin 1.0\nmarvin.settings.openai_api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_OPENAI_API_KEY</code>. </p> <p>To set your OpenAI API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_OPENAI_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p> <p>Using OpenAI standard API key locations</p> <p>For convenience, Marvin will respect the <code>OPENAI_API_KEY</code> environment variable or a key manually set as <code>openai.api_key</code> as long as no Marvin-specific keys were also provided.</p>"},{"location":"configuration/providers/#using-a-model","title":"Using a model","text":"<p>Once your API key is set, you can use any valid OpenAI model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\n\nmarvin.settings.llm_model = 'openai/gpt-4-0613'\n</code></pre></p>"},{"location":"configuration/providers/#anthropic","title":"Anthropic","text":"<p>Available in Marvin 1.1</p> <p>Marvin supports Anthropic's Claude 1 and Claude 2 models. In order to use the Anthropic API, you must provide an API key.</p> <p>Installing the Anthropic provider</p> <p>To use the Anthropic provider, you must have the <code>anthropic</code> Python client installed. You can do this by installing Marvin as <code>pip install \"marvin[anthropic]\"</code></p> <p>Anthropic is not optimized for calling functions</p> <p>Anthropic's models are not fine-tuned for calling functions or generating structured outputs. Therefore, Marvin adds a significant number of additional instructions to get Anthropic models to mimic this behavior. Empirically, this works very well for most Marvin components, including functions, models, and classifiers. However, it may not perform as well for more complex AI Applications.</p>"},{"location":"configuration/providers/#configuration_1","title":"Configuration","text":"<p>To use Anthropic models, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_ANTHROPIC_API_KEY</code> <code>marvin.settings.anthropic.api_key</code> \u2705"},{"location":"configuration/providers/#getting-an-api-key_1","title":"Getting an API key","text":"<p>To obtain an Anthropic API key, follow these steps:</p> <ol> <li>Log in to your Anthropic account (sign up if you don't have one)</li> <li>Go to the \"API Keys\" page under your account settings.</li> <li>Click \"Create Key.\" A new API key will be generated. Make sure to copy the key to your clipboard, as you will not be able to see it again.</li> </ol>"},{"location":"configuration/providers/#setting-the-api-key_1","title":"Setting the API key","text":"<p>You can set your API key at runtime like this:</p> <pre><code>import marvin\n\nmarvin.settings.anthropic.api_key = YOUR_API_KEY\n</code></pre> <p>However, it is preferable to pass sensitive settings as an environment variable: <code>MARVIN_ANTHROPIC_API_KEY</code>.</p> <p>To set your Anthropic API key as an environment variable, open your terminal and run the following command, replacing  with the actual key: <pre><code>export MARVIN_ANTHROPIC_API_KEY=&lt;your API key&gt;\n</code></pre> <p>This will set the key for the duration of your terminal session. To set it more permanently, configure your terminal or its respective env files.</p>"},{"location":"configuration/providers/#using-a-model_1","title":"Using a model","text":"<p>Once your API key is set, you can use any valid Anthropic model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\n\nmarvin.settings.llm_model = 'claude-2'\n</code></pre></p> <p>Marvin will automatically recognize that the <code>claude-*</code> family of models use the Anthropic provider. To indicate a provider explicitly, prefix the model name with <code>anthropic/</code>. For example: <code>marvin.settings.llm_model = 'anthropic/claude-2'</code>.</p>"},{"location":"configuration/providers/#azure-openai-service","title":"Azure OpenAI Service","text":"<p>Available in Marvin 1.1</p> <p>Marvin supports Azure's OpenAI service. In order to use the Azure service, you must provide an API key.</p>"},{"location":"configuration/providers/#configuration_2","title":"Configuration","text":"<p>To use the Azure OpenAI service, you can set the following configuration options:</p> Setting Env Variable Runtime Variable Required? Notes API key <code>MARVIN_AZURE_OPENAI_API_KEY</code> <code>marvin.settings.azure_openai.api_key</code> \u2705 API base <code>MARVIN_AZURE_OPENAI_API_BASE</code> <code>marvin.settings.azure_openai.api_base</code> \u2705 The API endpoint; this should have the form <code>https://YOUR_RESOURCE_NAME.openai.azure.com</code> Deployment name <code>MARVIN_AZURE_OPENAI_DEPLOYMENT_NAME</code> <code>marvin.settings.azure_openai.deployment_name</code> \u2705 API type <code>MARVIN_AZURE_OPENAI_API_TYPE</code> <code>marvin.settings.azure_openai.api_type</code> Either <code>azure</code> (the default) or <code>azure_ad</code> (to use Microsoft Active Directory to authenticate to your Azure endpoint)."},{"location":"configuration/providers/#using-a-model_2","title":"Using a model","text":"<p>Once the provider is configured, you can use any Azure OpenAI model by providing it as Marvin's <code>llm_model</code> setting: <pre><code>import marvin\n\nmarvin.settings.llm_model = 'azure_openai/gpt-35-turbo'\n</code></pre></p>"},{"location":"configuration/settings/","title":"Settings","text":"<p>Marvin makes use of Pydantic's <code>BaseSettings</code> for configuration throughout the package.</p>"},{"location":"configuration/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings are configurable via environment variables like <code>MARVIN_&lt;setting name&gt;</code>.</p> <p>For example, in an <code>.env</code> file or in your shell config file you might have: <pre><code>MARVIN_LOG_LEVEL=DEBUG\nMARVIN_LLM_MODEL=openai/gpt-4\nMARVIN_LLM_TEMPERATURE=0\n</code></pre></p>"},{"location":"configuration/settings/#runtime-settings","title":"Runtime Settings","text":"<p>A runtime settings object is accessible via <code>marvin.settings</code> and can be used to access or update settings throughout the package.</p> <p>For example, to access or change the LLM model used by Marvin at runtime: <pre><code>import marvin\n\nmarvin.settings.llm_model\n# 'openai/gpt-4'\n\nmarvin.settings.llm_model = 'openai/gpt-3.5-turbo'\n\nmarvin.settings.llm_model\n# 'openai/gpt-3.5-turbo'\n</code></pre></p>"},{"location":"configuration/settings/#llm-providers","title":"LLM Providers","text":"<p>Marvin supports multiple LLM providers, including OpenAI and Anthropic. After configuring your credentials appropriately, you can use any supported model by setting <code>marvin.settings.llm_model</code> appropriately. </p> <p>Valid <code>llm_model</code> settings are strings with the form <code>\"{provider_key}/{model_name}\"</code>. For example, <code>\"openai/gpt-3.5-turbo\"</code>, <code>anthropic/claude-2</code>, or <code>azure_openai/gpt-4</code>.</p> Provider Provider Key Models Notes OpenAI <code>openai</code> <code>gpt-3.5-turbo</code>, <code>gpt-4</code> (default), or any other compatible model Marvin is generally tested and optimized with this provider. Anthropic <code>anthropic</code> <code>claude-2</code>, <code>claude-instant-1</code> or any other compatible model Available in Marvin 1.1 Azure OpenAI Service <code>azure_openai</code> <code>gpt-3.5-turbo</code>, <code>gpt-4</code>, or any other compatible model The Azure OpenAI Service shares all the same configuration options as the OpenAI models, as well as a few additional ones. Available in Marvin 1.1."},{"location":"configuration/settings/#llm-configuration","title":"LLM Configuration","text":"<p>To configure LLM models globally, you can adjust the following settings. Note that these become the default settings for all models, but you can always set these on a per-model or per-component basis.</p> Setting Env Variable Runtime Variable Default Notes LLM model <code>MARVIN_LLM_MODEL</code> <code>marvin.settings.llm_model</code> <code>openai/gpt-3.5-turbo</code> Set the model as <code>{provider}/{model}</code>. Defaults to OpenAI's GPT-3.5 model. Temperature <code>MARVIN_LLM_TEMPERATURE</code> <code>marvin.settings.llm_temperature</code> 0.8 Max tokens <code>MARVIN_LLM_MAX_TOKENS</code> <code>marvin.settings.llm_max_tokens</code> 1500 The maximum number of tokens in a model completion Timeout <code>MARVIN_LLM_REQUEST_TIMEOUT_SECONDS</code> <code>marvin.settings.llm_request_timeout_seconds</code> 600.0"},{"location":"deployment/","title":"Index","text":""},{"location":"deployment/#fastapi","title":"FastAPI","text":"<p>We strongly recommend deploying Marvin's components with FastAPI. Here's how you can deploy a declarative API gateway in a few lines of code.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, ai_model\nfrom pydantic import BaseModel\nimport uvicorn\nimport asyncio\n\napp = FastAPI()\n\n\n@ai_fn\ndef generate_fruits(n: int) -&gt; list[str]:\n    \"\"\"Generates a list of `n` fruits\"\"\"\n\n\n@ai_fn\ndef generate_vegetables(n: int, color: str) -&gt; list[str]:\n    \"\"\"Generates a list of `n` vegetables of color `color`\"\"\"\n\n\n@ai_model\nclass Person(BaseModel):\n    first_name: str\n    last_name: str\n\n\napp.add_api_route(\"/generate_fruits\", generate_fruits)\napp.add_api_route(\"/generate_vegetables\", generate_vegetables)\napp.add_api_route(\"/person/extract\", Person.route())\n</code></pre> <p>If you want to serve the previous example from, say, a Jupyter Notebook for local testing, you can also include:</p> <pre><code># ... from above\n# If you want to run an API from a Jupyter Notebook.\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nawait server.serve()\n\n# Then navigate to localhost:8000/docs\n</code></pre>"},{"location":"examples/classification_api/","title":"Basic Classifier API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to classify data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/classification_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that classifies a paragraph of text into predefined categories.  This is typically a pretty daunting task for a machine learning savant, much less your average engineer.  Marvin lets you accomplish this by writing code the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>str</code>, or string. With Marvin, we'll use <code>ai_fn</code> and decorate this function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>classify_text('The Lakers won the game last night')\n\n# returns Category.SPORTS\n</code></pre>"},{"location":"examples/classification_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\nfrom typing import Literal\n\nsettings.openai.api_key = 'API_KEY' \n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/classify_text/\")\n@ai_fn\ndef classify_text(text: str) -&gt; Literal['sports', 'politics', 'technology']:\n    '''\n        Correctly classifies the passed `text` into one of the predefined categories. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/classify_text/</code> endpoint and get the classified category they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"The Lakers won the game last night\"}\nresponse = requests.post(\"http://localhost:8000/classify_text/\", json=data)\nprint(response.json())\n\n# returns 'SPORTS'\n</code></pre> This will send a POST request to the <code>/classify_text/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/deduplication/","title":"Entity Deduplication","text":"<p>What is entity deduplication?</p> <p>How many distinct cities are mentioned in the following text:</p> <p>Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco. </p> <p>We know it's three, but getting software to deduplicate these entities is surprisingly hard. </p> <p>How can we turn it into something cleaner like:</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>In this example, we'll explore how you can do text and entity deduplication from a piece of text. </p>"},{"location":"examples/deduplication/#creating-our-data-model","title":"Creating our data model","text":"<p>To extract and deduplicate entities, we'll want to think carefully about the data we want to extract from this text. We clearly want a <code>list</code> of <code>cities</code>. So we'll want to create a data model to represent a city. But we won't stop there:  we don't want to just get a list of cities that appear in the text. We want to get an mapping or understanding that SF is the same as San Francisco, and the Big Apple is the same as New York City, etc. </p> <pre><code>import pydantic\n\nclass City(pydantic.BaseModel):\n    '''\n        A model to represent a city.\n    '''\n\n    text: str = pydantic.Field(\n        description = 'The city name as it appears'\n    )\n\n    inferred_city: str = pydantic.Field(\n            description = 'The inferred and normalized city name.'\n        )\n</code></pre>"},{"location":"examples/deduplication/#creating-our-prompt","title":"Creating our prompt","text":"<p>Now we'll need to use this model and convert it into a prompt we can send to a language model. We'll use Marvin's prompt_fn to let us write a prompt like a python function. </p> <pre><code>from marvin import prompt_fn\n\n@prompt_fn\ndef get_cities(text: str) -&gt; list[City]:\n    '''\n        Expertly deduce and infer all cities from the follwing text: {{ text }}\n    '''\n</code></pre> What does get_cities do under the hood? <p>Marvin's <code>prompt_fn</code> only creates a prompt to send to a large language model. It does not call any  external service, it's simply responsible for translating your query into something that a  large language model will understand. </p> <p>Here's the output when we plug in our sentence from above:</p> <pre><code>get_cities(\"Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco.\")\n</code></pre> Click to see output <pre><code>{\n\"messages\": [\n    {\n    \"role\": \"system\",\n    \"content\": \"Expertly deduce and infer all cities from the follwing text: Chicago, The Windy City, New York City, the Big Apple, SF, San Fran, San Francisco.\"\n    }\n],\n\"functions\": [\n    {\n    \"parameters\": {\n        \"$defs\": {\n        \"City\": {\n            \"description\": \"A model to represent a city.\",\n            \"properties\": {\n            \"text\": {\n                \"description\": \"The city name as it appears\",\n                \"title\": \"Text\",\n                \"type\": \"string\"\n            },\n            \"inferred_city\": {\n                \"description\": \"The inferred and normalized city name.\",\n                \"title\": \"Inferred City\",\n                \"type\": \"string\"\n            }\n            },\n            \"required\": [\n            \"text\",\n            \"inferred_city\"\n            ],\n            \"title\": \"City\",\n            \"type\": \"object\"\n        }\n        },\n        \"properties\": {\n        \"output\": {\n            \"items\": {\n            \"$ref\": \"#/$defs/City\"\n            },\n            \"title\": \"Output\",\n            \"type\": \"array\"\n        }\n        },\n        \"required\": [\n        \"output\"\n        ],\n        \"type\": \"object\"\n    },\n    \"name\": \"Output\",\n    \"description\": \"\"\n    }\n],\n\"function_call\": {\n    \"name\": \"Output\"\n}\n}\n</code></pre>"},{"location":"examples/deduplication/#calling-our-language-model","title":"Calling our Language Model","text":"<p>Let's see what happens when we actually call our Large Language Model. Below, <code>**</code> tells let's us pass the prompt's parameters into our call to OpenAI.</p> <pre><code>import openai\nimport json\n\nresponse = openai.ChatCompletion.create(\n    api_key = 'YOUR OPENAI KEY',\n    model = 'gpt-3.5-turbo',\n    temperature = 0,\n    **get_cities(\n        (\n            \"Chicago, The Windy City, New York City, \"\n            \"The Big Apple, SF, San Fran, San Francisco.\"\n        )\n    )\n)\n</code></pre> View the raw response <p>The raw response we receive looks like  <pre><code>{\n    \"id\": \"omitted for this example\",\n    \"object\": \"chat.completion\",\n    \"created\": 1697222527,\n    \"model\": \"gpt-3.5-turbo-0613\",\n    \"choices\": [\n        {\n        \"index\": 0,\n        \"message\": {\n            \"role\": \"assistant\",\n            \"content\": null,\n            \"function_call\": {\n            \"name\": \"Output\",\n            \"arguments\": \"{\\n  \\\"output\\\": [\\n    {\\n      \\\"text\\\": \\\"Chicago\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Windy City\\\",\\n      \\\"inferred_city\\\": \\\"Chicago\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"New York City\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"The Big Apple\\\",\\n      \\\"inferred_city\\\": \\\"New York City\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"SF\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Fran\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    },\\n    {\\n      \\\"text\\\": \\\"San Francisco\\\",\\n      \\\"inferred_city\\\": \\\"San Francisco\\\"\\n    }\\n  ]\\n}\"\n            }\n        },\n        \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 87,\n        \"completion_tokens\": 165,\n        \"total_tokens\": 252\n    }\n}\n</code></pre></p> <p>We can parse the raw response and mine out the relevant responses, </p> <pre><code>[\n    City.parse_obj(city) \n    for city in \n    json.loads(\n        response.choices[0].message.function_call.arguments\n    ).get('output')\n]\n</code></pre> <p>what we'll get now is the pairs of raw, observed city and cleaned deduplicated city.</p> <pre><code>[\n    City(text='Chicago', inferred_city='Chicago'),\n    City(text='The Windy City', inferred_city='Chicago'),\n    City(text='New York City', inferred_city='New York City'),\n    City(text='The Big Apple', inferred_city='New York City'),\n    City(text='SF', inferred_city='San Francisco'),\n    City(text='San Fran', inferred_city='San Francisco'),\n    City(text='San Francisco', inferred_city='San Francisco')\n]\n</code></pre> <p>So, we've seen that deduplicating data with a Large Language Model is fairly straightforward in a customizable way using Marvin. If you want the entire content of the cells above in  one place, you can copy the cell below.</p> Copy <pre><code>import openai\nimport json\nimport pydantic\nfrom marvin import prompt_fn\n\nclass City(pydantic.BaseModel):\n    '''\n        A model to represent a city.\n    '''\n\n    text: str = pydantic.Field(\n        description = 'The city name as it appears'\n    )\n\n    inferred_city: str = pydantic.Field(\n            description = 'The inferred and normalized city name.'\n        )\n\n@prompt_fn\ndef get_cities(text: str) -&gt; list[City]:\n    '''\n        Expertly deduce and infer all cities from the follwing text: {{ text }}\n    '''\n\nresponse = openai.ChatCompletion.create(\n    api_key = 'YOUR OPENAI KEY',\n    model = 'gpt-3.5-turbo',\n    temperature = 0,\n    **get_cities(\n        (\n            \"Chicago, The Windy City, New York City, \"\n            \"The Big Apple, SF, San Fran, San Francisco.\"\n        )\n    )\n)\n\n[\n    City.parse_obj(city) \n    for city in \n    json.loads(\n        response.choices[0].message.function_call.arguments\n    ).get('output')\n]\n</code></pre>"},{"location":"examples/extraction_api/","title":"Basic Extraction API","text":"<p>With Marvin, you can easily build a production-grade application or data pipeline to extract structured data from unstructured text.  In this example, we'll show how to </p> <ul> <li>Write an AI-powered Function.</li> <li>Build a production-ready API.</li> </ul>"},{"location":"examples/extraction_api/#writing-an-ai-powered-function","title":"Writing an AI-powered Function.","text":"<p>Example</p> <p>Marvin translates your Python code into English, passes that to an Large Language Model,  and parses its response. It uses AI to evaluate your function, no code required.</p> <p>Let's build a function that extracts a person's first_name, last_name, and age  from a paragraph of text. This is typically a pretty daunting task for a machine learning savant, much less your average engineer. Marvin lets you accomplish this by write code  the way you normally would, no PhD required.</p> <p>We'll simply write a Python function, tell it that we expect a <code>text</code> input and that it'll  output a <code>dict</code>, or dictionary. With Marvin, we'll use <code>ai_fn</code> and decorate his function.  When we do, this function will use AI to get its answer.</p> <p><pre><code>from marvin import ai_fn, settings\nfrom typing import Any\n\nsettings.openai.api_key = 'API_KEY' \n\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> This function can now be run! When we test it out, we get great results.</p> Results <pre><code>extract_person('My name is Peter Parker, and I was born when Clinton was first elected')\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre>"},{"location":"examples/extraction_api/#build-a-production-ready-api","title":"Build a production-ready API.","text":"<p>In the following example, we will demonstrate how to deploy the AI function we just created as an API using FastAPI. FastAPI is a powerful tool that allows us to easily turn our AI function into a fully-fledged API. This API can then be used by anyone to send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need. Let's see how this can be done.</p> <p>Example</p> <p>Now that we have our AI function, let's deploy it as an API using FastAPI. FastAPI is a modern, fast (high-performance), web framework for building APIs.</p> <pre><code>from fastapi import FastAPI\nfrom marvin import ai_fn, settings\n\napp = FastAPI()\n\nsettings.openai.api_key = 'API_KEY'\n\n@app.post(\"/extract_person/\")\n@ai_fn\ndef extract_person(text: str) -&gt; dict:\n    '''\n        Correctly infers a persons `birth_year`, `first_name` and `last_name`\n        from the passed `text`. \n    '''\n</code></pre> <p>With just a few lines of code, we've turned our AI function into a fully-fledged API. Now, anyone can send a POST request to our <code>/extract_person/</code> endpoint and get the structured data they need.</p> API Deployment <p><pre><code>import uvicorn\nimport asyncio\n\nconfig = uvicorn.Config(app)\nserver = uvicorn.Server(config)\nasyncio.run(server.serve())\n</code></pre> Now, you can navigate to localhost:8000/docs to interact with your API.</p> Making Requests <p><pre><code>import requests\n\ndata = {\"text\": \"My name is Peter Parker, and I was born when Clinton was first elected\"}\nresponse = requests.post(\"http://localhost:8000/extract_person/\", json=data)\nprint(response.json())\n\n# returns {'first_name': 'Peter', 'last_name': 'Parker', 'birth_year': 1992}\n</code></pre> This will send a POST request to the <code>/extract_person/</code> endpoint with the provided text and print the response.</p>"},{"location":"examples/github_digest/","title":"GitHub Digest","text":"<p>A fun example covering a few practical patterns is to create an AI digest of GitHub activity for a given repo.</p> <p>If you've spent some time messing with AI tools in the Python ecosystem lately, you're probably familiar with Jinja2. Jinja pairs really nicely with LLMs, because you can structure the template in a way that either makes it easy for the LLM to fill in the blanks, or makes it easy for you to fill in the blanks with traditional software and then pass the rendered template to the LLM as a prompt.</p> <p>Here's an example of the latter:</p>"},{"location":"examples/github_digest/#writing-an-epic-about-the-days-events-in-prefecthqprefect","title":"Writing an epic about the day's events in <code>PrefectHQ/prefect</code>","text":"<p>The AI part is pretty much English:</p> <pre><code>@ai_fn(\n    instructions=\"You are a witty and subtle orator. Speak to us of the day's events.\"\n)\ndef summarize_digest(markdown_digest: str) -&gt; str:\n    \"\"\"Given a markdown digest of GitHub activity, create a Story that is\n    informative, entertaining, and epic in proportion to the day's events -\n    an empty day should be handled with a short sarcastic quip about humans\n    and their laziness.\n\n    The story should capture collective efforts of the project.\n    Each contributor plays a role in this story, their actions\n    (issues raised, PRs opened, commits merged) shaping the events of the day.\n\n    The narrative should highlight key contributors and their deeds, drawing upon the\n    details in the digest to create a compelling and engaging tale of the day's events.\n    A dry pun or 2 are encouraged.\n\n    Usernames should be markdown links to the contributor's GitHub profile.\n\n    The story should begin with a short pithy welcome to the reader and have\n    a very short, summarizing title.\n    \"\"\" # noqa: E501 (to make Ruff happy)\n</code></pre> <p>... and the rest is just... Python?</p> <pre><code>import os\nimport inspect\nfrom datetime import date, datetime, timedelta\n\nfrom marvin import ai_fn\n\nfrom my_helpers import (\n    fetch_contributor_data,\n    post_slack_message,\n    YOUR_JINJA_TEMPLATE,\n)\n\nasync def daily_github_digest(\n    owner: str = \"PrefectHQ\",\n    repo: str = \"marvin\",\n    slack_channel: str = \"ai-tools\",\n    gh_token_env_var: str = \"GITHUB_PAT\",\n):\n    since = datetime.utcnow() - timedelta(days=1)\n\n    data = await fetch_contributor_data(\n        token=os.getenv(gh_token_env_var), # load from your secrets manager\n        owner=owner,\n        repo=repo,\n        since=since,\n    )\n\n    markdown_digest = YOUR_JINJA_TEMPLATE.render(\n        today=date.today(),\n        owner=owner,\n        repo=repo,\n        contributors_activity=data,\n    )\n\n    epic_story = summarize_digest(markdown_digest)\n\n    await post_slack_message(\n        message=epic_story,\n        channel=slack_channel,\n    )\n\n\nif __name__ == \"__main__\":\n    import asyncio\n    asyncio.run(daily_github_digest(owner=\"PrefectHQ\", repo=\"prefect\"))\n</code></pre> <p>Tip</p> <p>I brought some helpers with me to make this easier:</p> <ul> <li><code>fetch_contributor_data</code> uses the GitHub API to get a list of contributors and their activity</li> <li><code>post_slack_message</code> uses the Slack API to post a message to a channel</li> <li><code>YOUR_JINJA_TEMPLATE</code> is a Jinja template that you can use to render the digest</li> </ul> <p>Find my helpers here</p> <p>Here's a sample output from August 16th 2023 on the <code>PrefectHQ/prefect</code> repo: <pre><code>Greetings, wanderer! Sit, rest your feet, and allow me to regale you with the\nheroic deeds of the PrefectHQ/prefect repository on this fateful day, the 16th\nof August, 2023.\n\nOur tale begins with the indefatigable jakekaplan, who single-handedly opened \ntwo perceptive Pull Requests: flow run viz v2 and don't run tasks draft-. Not\nsatisfied, he also valiantly merged five commits, clearing the path for others\nto tread. Meanwhile, cicdw, desertaxle, and serinamarie, each merged a single,\nbut significant commit, contributing their mite to the collective effort.\n\nIn the realm of PRs, prefectcboyd, WillRaphaelson, and bunchesofdonald all \nunfurled their banners, each opening a PR of their own, like pioneers staking\ntheir claim on the wild frontiers of code.\n\nIn the grand theatre of software development, where victory is measured in merged\ncommits and opened PRs, these individuals stood tall, their actions resonating \nthrough the corridors of GitHub. Their names are etched onto the scroll of this \nday, their deeds a testament to their commitment.\n\nSo ends the telling of this day's events. Until our paths cross again, wanderer, \nmay your code compile and your tests always pass!\n</code></pre></p>"},{"location":"examples/github_digest/#full-example","title":"Full example:","text":"<p>The full example, with improvements like caching and observability, can be found here.</p>"},{"location":"examples/slackbot/","title":"Build a Slack bot with Marvin","text":""},{"location":"examples/slackbot/#slack-setup","title":"Slack setup","text":"<p>Get a Slack app token from Slack API and add it to your <code>.env</code> file:</p> <pre><code>MARVIN_SLACK_API_TOKEN=your-slack-bot-token\n</code></pre> <p>Choosing scopes</p> <p>You can choose the scopes you need for your bot in the OAuth &amp; Permissions section of your Slack app.</p>"},{"location":"examples/slackbot/#building-the-bot","title":"Building the bot","text":""},{"location":"examples/slackbot/#define-a-message-handler","title":"Define a message handler","text":"<p><pre><code>import asyncio\nfrom fastapi import HTTPException\n\nasync def handle_message(payload: dict) -&gt; dict:\n    event_type = payload.get(\"type\", \"\")\n\n    if event_type == \"url_verification\":\n        return {\"challenge\": payload.get(\"challenge\", \"\")}\n    elif event_type != \"event_callback\":\n        raise HTTPException(status_code=400, detail=\"Invalid event type\")\n\n    asyncio.create_task(generate_ai_response(payload))\n\n    return {\"status\": \"ok\"}\n</code></pre> Here, we define a simple python function to handle Slack events and return a response. We run our interesting logic in the background using <code>asyncio.create_task</code> to make sure we return <code>{\"status\": \"ok\"}</code> within 3 seconds, as required by Slack.</p>"},{"location":"examples/slackbot/#implement-the-ai-response","title":"Implement the AI response","text":"<p>I like to start with this basic structure, knowing that one way or another...</p> <pre><code>async def generate_ai_response(payload: dict) -&gt; str:\n    # somehow generate the ai responses\n    ...\n\n    # post the response to slack\n    _post_message(\n        messsage=some_message_ive_constructed,\n        channel=event.get(\"channel\", \"\"),\n        thread_ts=thread_ts,\n    )\n</code></pre> <p>... I need to take in a Slack app mention payload, generate a response, and post it back to Slack.</p>"},{"location":"examples/slackbot/#a-couple-considerations","title":"A couple considerations","text":"<ul> <li>do I want the bot to respond to users in a thread or in the channel?</li> <li>do I want the bot to have memory of previous messages? how so?</li> <li>what tools do I need to generate accurate responses for my users?</li> </ul> <p>In our case of the Prefect Community slackbot, we want:</p> <ul> <li>the bot to respond in a thread</li> <li>the bot to have memory of previous messages by slack thread</li> <li>the bot to have access to the internet, GitHub, embedded docs, a calculator, and have the ability to immediately save useful slack threads to Discourse for future reference by the community</li> </ul>"},{"location":"examples/slackbot/#implementation-of-generate_ai_response-for-the-prefect-community-slackbot","title":"Implementation of <code>generate_ai_response</code> for the Prefect Community Slackbot","text":"<p>Here we invoke a worker <code>Chatbot</code> that has the <code>tools</code> needed to generate an accurate and helpful response.</p> <pre><code>async def generate_ai_response(payload: dict) -&gt; str:\n    event = payload.get(\"event\", {})\n    channel_id = event.get(\"channel\", \"\")\n    message = event.get(\"text\", \"\")\n\n    bot_user_id = payload.get(\"authorizations\", [{}])[0].get(\"user_id\", \"\")\n\n    if match := re.search(SLACK_MENTION_REGEX, message):\n        thread_ts = event.get(\"thread_ts\", \"\")\n        ts = event.get(\"ts\", \"\")\n        thread = thread_ts or ts\n\n        mentioned_user_id = match.group(1)\n\n        if mentioned_user_id != bot_user_id:\n            get_logger().info(f\"Skipping message not meant for the bot: {message}\")\n            return\n\n        message = re.sub(SLACK_MENTION_REGEX, \"\", message).strip()\n        # `CACHE` is a TTL cache that stores a `History` object for each thread\n        history = CACHE.get(thread, History())\n\n        bot = choose_bot(payload=payload, history=history)\n\n        ai_message = await bot.run(input_text=message)\n\n        CACHE[thread] = deepcopy(\n            bot.history\n        )  # make a copy so we don't cache a reference to the history object\n\n        message_content = _clean(ai_message.content)\n\n        await post_slack_message(\n            message=message_content,\n            channel=channel_id,\n            thread_ts=thread,\n        )\n\n        return message_content\n</code></pre> <p>This is just an example</p> <p>Find my specific helpers here.</p> <p>Unlike previous version of <code>marvin</code>, we don't necessarily have a database full of historical messages to pull from for a thread-based history. Instead, we'll cache the histories in memory for the duration of the app's runtime. Thread history can / should be implemented in a more robust way for specific use cases.</p>"},{"location":"examples/slackbot/#attach-our-handler-to-a-deployable-aiapplication","title":"Attach our handler to a deployable <code>AIApplication</code>","text":"<p>All Marvin components are directly deployable as FastAPI applications - check it out: <pre><code>from chatbot import handle_message\nfrom marvin import AIApplication\nfrom marvin.deployment import Deployment\n\ndeployment = Deployment(\n    component=AIApplication(tools=[handle_message]),\n    app_kwargs={\n        \"title\": \"Marvin Slackbot\",\n        \"description\": \"A Slackbot powered by Marvin\",\n    },\n    uvicorn_kwargs={\n        \"port\": 4200,\n    },\n)\n\nif __name__ == \"__main__\":\n    deployment.serve()\n</code></pre></p> <p>Deployments</p> <p>Learn more about deployments here.</p> <p>Run this file with something like:</p> <pre><code>python slackbot.py\n</code></pre> <p>... and navigate to <code>http://localhost:4200/docs</code> to see your bot's docs.</p> <p>This is now an endpoint that can be used as a Slack event handler. You can use a tool like ngrok to expose your local server to the internet and use it as a Slack event handler.</p>"},{"location":"examples/slackbot/#building-an-image","title":"Building an image","text":"<p>Based on this example, one could write a <code>Dockerfile</code> to build a deployable image:</p> <p><pre><code>FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY . /app\n\nRUN python -m venv venv\nENV VIRTUAL_ENV=/app/venv\nENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n\nRUN apt-get update &amp;&amp; apt-get install -y git\n\nRUN pip install \".[slackbot,ddg]\"\n\nEXPOSE 4200\n\nCMD [\"python\", \"cookbook/slackbot/start.py\"]\n</code></pre> Note that we're installing the <code>slackbot</code> and <code>ddg</code> extras here, which are required for tools used by the worker bot defined in this example's <code>cookbook/slackbot/start.py</code> file.</p>"},{"location":"examples/slackbot/#find-the-whole-example-here","title":"Find the whole example here.","text":""},{"location":"llms/llms/","title":"Calling LLMs","text":"<p>Marvin has a simple API for working with LLMs that can be used with all supported LLM providers. The LLM API is designed to be a drop-in replacement for OpenAI's Python SDK, with additional functionality to improve user experience.</p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"llms/llms/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\n\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI (after calling .response)</p> <pre><code>print(openai.response.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\n\nprint(anthropic.response.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n\n# Call the instance with create. \nopenai_pirate.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\n\n\nopenai.ChatCompletion().create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre>"},{"location":"llms/llms/#advanced-use","title":"Advanced Use","text":""},{"location":"llms/llms/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass CoffeeOrder(BaseModel):\n    size: Literal['small', 'medium', 'large']\n    milk: Literal['soy', 'oat', 'dairy']\n    with_sugar: bool = False\n\n\nresponse = openai.ChatCompletion().create(\n    messages = [{\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Translation(BaseModel):\n    spanish: str\n    french: str\n    swedish: str\n\n\nresponse = openai.ChatCompletion().create(\n    messages = [\n    {\n        'role': 'system',\n        'content': 'You translate user messages into other languages.'\n    },\n    {\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"llms/llms/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = ZZZcreate(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put it $100 every month for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n    \"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\n    A = P * (1 + r/n)**(n*t)\n    return round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put in $50/mo for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"llms/llms/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\n\ndef divide(x: float, y: float) -&gt; str:\n    '''Divides x and y'''\n    return str(x/y)\n\ndef add(x: int, y: int) -&gt; str:\n    '''Adds x and y'''\n    return str(x+y)\n\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n\n    # Start off with an external question / prompt. \n    prompt = 'What is 4124124 + 424242 divided by 48124?'\n\n    # Initialize the conversation with a prompt from the user. \n    conversation.send(messages = [{'role': 'user', 'content': prompt}])\n\n    # While the most recent turn has a function call, evaluate it. \n    while conversation.last_turn.has_function_call():\n\n        # Send the most recent function call to the conversation. \n        conversation.send(messages = [\n            conversation.last_turn.call_function() \n        ])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_turn.choices[0].message.content\n\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[turn.response.choices[0].message.dict() for turn in conversation.turns]\n\n[\n{\n    \"content\": null,\n    \"role\": \"assistant\",\n    \"name\": null,\n    \"function_call\": {\n    \"name\": \"add\",\n    \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n    }\n},\n{\n    \"content\": null,\n    \"role\": \"assistant\",\n    \"name\": null,\n    \"function_call\": {\n    \"name\": \"divide\",\n    \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n    }\n},\n{\n    \"content\": \"4124124 + 424242 divided by 48124 is approximately 94.51346521486161.\",\n    \"role\": \"assistant\",\n    \"name\": null,\n    \"function_call\": null\n}\n]\n</code></pre></p>"},{"location":"llms/prompt/","title":"Building Prompts","text":""},{"location":"llms/prompt/#creating-prompts","title":"Creating Prompts","text":"<p>Marvin lets you define dynamic prompts using code, eliminating the need for cumbersome template management. With this approach, you can easily create reusable and modular prompts, streamlining the development process.</p> <p>Example</p> <pre><code>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{ topic }}. \"\n        \"When asked questions about {{ topic }}, you answer correctly.\"\n    )\n    topic: Optional[str]\n\n\nprompt = (\n    ExpertSystem(topic=\"python\")\n    | User(\"Write a function to find the nth Fibonacci number.\")\n    | ChainOfThought()  # Tell the LLM to think step by step\n)\n\n# We can now call `dict` to get the formatted messages.\nprompt.dict()\n</code></pre> Click to see output <pre><code>[\n    {\n        'role': 'system',\n        'content': 'You are a world-class expert on python. \n                    When asked questions about python, you \n                    answer correctly.'\n    },\n    {\n        'role': 'user',\n        'content': 'I need to know how to write a function to\n                    find the nth Fibonacci number.'\n    },\n    {  'role': 'assistant', \n        'content': \"Let's think step by step.\"\n    }\n]\n</code></pre>"},{"location":"llms/prompt/#templating-prompts","title":"Templating Prompts","text":"<p>In many applications, templating is unavoidable. In these cases, Marvin's optional templating engine simplifies the process of sharing context across prompts to an unprecedented level. By passing native Python types or Pydantic objects into the rendering engine, you can seamlessly establish context for entire conversations. This feature enables effortless information flow and context continuity throughout the prompt interactions.</p> <p>Example</p> <pre><code>from typing import Optional\nfrom marvin.prompts.library import System, User, ChainOfThought\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{ topic }}. \"\n        \"When asked questions about {{ topic }}, you answer correctly.\"\n    )\n    topic: Optional[str]\n\n\nprompt = (\n    ExpertSystem()\n    | User(\n        \"I need to know how to write a function in {{ topic }} to find the nth Fibonacci \"\n        \"number.\"\n    )\n    | ChainOfThought()  # Tell the LLM to think step by step\n)\n# We can now call `dict` with keyword arguments to get the formatted messages.\nprompt.dict(topic=\"rust\")\n</code></pre> Click to see output <pre><code>[\n    {\n        'role': 'system',\n        'content': 'You are a world-class expert on rust. \n                    When asked questions about rust, you answer correctly.'\n    },\n    {\n        'role': 'user',\n        'content': 'I need to know how to write a function in \n                    rust to find the nth Fibonacci number.'\n    },\n    {\n        'role': 'assistant', \n        'content': \"Let's think step by step.\"\n    }\n]\n</code></pre>"},{"location":"llms/prompt/#example-react","title":"Example: ReAct","text":"<p>Example</p> <pre><code>from marvin.prompts.library import System\n\n\nclass ReActPattern(System):\n    content = \"\"\"\n    You run in a loop of Thought, Action, PAUSE, Observation.\n    At the end of the loop you output an Answer\n    Use Thought to describe your thoughts about the question you have been asked.\n    Use Action to run one of the actions available to you - then return PAUSE.\n    Observation will be the result of running those actions.\n    \"\"\"\n</code></pre>"},{"location":"llms/prompt/#example-sql","title":"Example: SQL","text":"<p>Example</p> <pre><code>import pydantic\nfrom marvin.prompts.library import System\n\n\nclass ColumnInfo(pydantic.BaseModel):\n    name: str\n    description: str\n\n\nclass SQLTableDescription(System):\n    content = \"\"\"\n    If you chose to, you may query a table whose schema is defined below:\n\n\n    \"\"\"\n\n    columns: list[ColumnInfo] = pydantic.Field(\n        ..., description=\"name, description pairs of SQL Schema\"\n    )\n\n\nUserQueryPrompt = SQLTableDescription(\n    columns=[\n        ColumnInfo(name=\"last_login\", description=\"Date and time of user's last login\"),\n        ColumnInfo(\n            name=\"date_created\",\n            description=\"Date and time when the user record was created\",\n        ),\n        ColumnInfo(\n            name=\"date_last_purchase\",\n            description=\"Date and time of user's last purchase\",\n        ),\n    ]\n)\n\nprint(UserQueryPrompt.read())\n</code></pre> Click to see output <pre><code>If you chose to, you may query a table whose schema is defined below:\n\n- last_login: Date and time of user's last login\n- date_created: Date and time when the user record was created\n- date_last_purchase: Date and time of user's last purchase\n</code></pre>"},{"location":"llms/prompt/#executing-prompts","title":"Executing Prompts","text":"<p>Marvin makes executing one-off <code>task</code> or <code>chain</code> patterns dead simple. </p>"},{"location":"llms/prompt/#running-a-task","title":"Running a <code>task</code>","text":"<p>Once you have a prompt defined, fire it off with your chosen LLM asyncronously like so:</p> <p>Example</p> <pre><code>from marvin.prompts.library import System, User, ChainOfThought\nfrom marvin.engine.language_models import chat_llm\nfrom typing import Optional\n\n\nclass ExpertSystem(System):\n    content: str = (\n        \"You are a world-class expert on {{ topic }}. \"\n        \"When asked questions about {{ topic }}, you answer correctly. \"\n        \"You only answer questions about {{ topic }}. \"\n    )\n    topic: Optional[str]\n\n\nclass Tutor(System):\n    content: str = (\n        \"When you give an answer, you modulate your response based on the \"\n        \"inferred knowledge of the user. \"\n        \"Your student's name is {{ name }}. \"\n    )\n    name: str = \"not provided\"\n\n\nmodel = chat_llm()\n\nresponse = await model(\n    (\n        ExpertSystem()\n        | Tutor()\n        | User(\n            \"I heard that there are types of geometries when the angles don't add up to\"\n            \" 180?\"\n        )\n        | ChainOfThought()\n    ).render(topic=\"geometry\", name=\"Adam\")\n)\n\nprint(response.content)\n</code></pre> Click to see output <pre><code>Yes, you are correct! In traditional Euclidean geometry, \nthe angles of a triangle always add up to 180 degrees. \nHowever, there are indeed other types of geometries where \nthis is not the case. One such example is non-Euclidean \ngeometry, which includes hyperbolic and elliptic geometries. \nIn hyperbolic geometry, the angles of a triangle add up to \nless than 180 degrees, while in elliptic geometry, the angles \nadd up to more than 180 degrees. These non-Euclidean \ngeometries have their own unique properties and are \nstudied in mathematics and physics.\n</code></pre>"},{"location":"prompting/prompt_function/","title":"Prompt Function","text":"<p>Marvin puts the engineering in prompt engineering. We expose a low-level <code>prompt_fn</code> decorator that lets you write prompts as functions. This lets you build fully type-hinted prompts that other engineers can introspect, version, and test.</p> <p>This is the easiest way to use Azure / OpenAI's function calling API.</p>"},{"location":"prompting/prompt_function/#basic-use","title":"Basic Use","text":""},{"location":"prompting/prompt_function/#type-hinting","title":"Type Hinting","text":"<p>Example</p> <p>Marvin translates your Python code into English. We'll simply write a Python function,  tell it that we expect an integer input <code>n</code> input and that it'll  output <code>list[str]</code>, or a list of strings. With Marvin, we'll use <code>prompt_fn</code> and decorate this function.  When we do, this function can be cast to a payload that can be send to an LLM.</p> <p><pre><code>from marvin.prompts import prompt_fn\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"type\": \"string\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ]\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#advanced-use","title":"Advanced Use","text":""},{"location":"prompting/prompt_function/#use-with-pydantic","title":"Use with Pydantic","text":"<p>Example</p> <p>Marvin supports type-hinting with Pydantic, so your return annotation can be a more complex data-model.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"FruitList\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"FruitList\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#full-customization","title":"Full Customization","text":"<p>Example</p> <p>Marvin supports full customization of every element of your prompts. You can customize the  name, description, and field_names of your <code>response_model</code>. </p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(\n    response_model_name = 'Fruktlista', \n    response_model_description = 'A list of fruits in Swedish',\n    response_model_field_name = 'Frukt'\n)\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} fruits'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Generates a list of 3 blue fruits\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"Frukt\": {\n                \"title\": \"Frukt\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/Fruit\"\n                }\n            }\n            },\n            \"required\": [\n            \"Frukt\"\n            ],\n            \"definitions\": {\n            \"Fruit\": {\n                \"title\": \"Fruit\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Fruktlista\",\n        \"description\": \"A list of fruits in Swedish\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Fruktlista\"\n    }\n    }\n</code></pre>"},{"location":"prompting/prompt_function/#referencing-globals","title":"Referencing Globals","text":"<p>Example</p> <p>Marvin passes the name of your response model to the prompt for you to reference as a convenience.</p> <p>Say we want to change the task to generate in Swedish.</p> <p><pre><code>from marvin.prompts import prompt_fn\nfrom pydantic import BaseModel\n\nclass Fruit(BaseModel):\n    color: str\n\n@prompt_fn(response_model_name = 'Fruits')\ndef list_fruits(n: int, color: str = 'red') -&gt; list[Fruit]:\n    '''Generates a list of {{ n }} {{ color }} {{ response_model.__name__.lower() }}'''\n\nlist_fruits(3, color = 'blue')\n</code></pre> This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"$ref\": \"#/definitions/Fruit\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ],\n                \"definitions\": {\n                \"Fruit\": {\n                    \"title\": \"Fruit\",\n                    \"type\": \"object\",\n                    \"properties\": {\n                    \"color\": {\n                        \"title\": \"Color\",\n                        \"type\": \"string\"\n                    }\n                    },\n                    \"required\": [\n                    \"color\"\n                    ]\n                }\n                }\n            },\n            \"name\": \"Fruits\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Fruits\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#contexts","title":"Contexts","text":"<p>Example</p> <p>Marvin supports full passing context dictionaries to your prompt's rendering environment.</p> <p>Say we want to list 'seasonal' fruits. We'll pass the datetime.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    Generates a list of {{ n }} {{ color }} fruits in season.\n        - The date is {{ today }}\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"Generates a list of 3 blue fruits in season.\\n- The date is 2023-09-22\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#multi-turn-prompts","title":"Multi-Turn Prompts","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom datetime import date\n\n@prompt_fn(ctx = {'today': date.today()})\ndef list_fruits(n: int, color: str = 'red') -&gt; list[str]:\n    ''' \n    System: You generate a list of {{ n }} fruits in season.\n        - The date is {{ today }}\n\n    User: I want {{ color }} fruits only.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>list_fruits(3, color = 'blue')</code> <pre><code>    {\n        \"messages\": [\n            {\n            \"role\": \"system\",\n            \"content\": \"You generate a list of 3 fruits in season.\\n- The date is 2023-09-22\"\n            },\n            {\n            \"role\": \"user\",\n            \"content\": \"I want blue fruits.\"\n            }\n        ],\n        \"functions\": [\n            {\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"output\": {\n                    \"title\": \"Output\",\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"string\"\n                    }\n                }\n                },\n                \"required\": [\n                \"output\"\n                ]\n            },\n            \"name\": \"Output\",\n            \"description\": \"\"\n            }\n        ],\n        \"function_call\": {\n            \"name\": \"Output\"\n        }\n        }\n</code></pre>"},{"location":"prompting/prompt_function/#use-cases","title":"Use Cases","text":""},{"location":"prompting/prompt_function/#classification","title":"Classification","text":"<p>Example</p> <p>Marvin supports multi-turn conversations. If no role is specified, the whole block is assumed to be a system prompt. To override this default behavior, simply break into Human, System, Assistant turns. </p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import Optional\nfrom enum import Enum\n\nclass Food(Enum):\n    '''\n        Food classes\n    '''\n    FRUIT = 'Fruit'\n    VEGETABLE = 'Vegetable'\n\n@prompt_fn\ndef classify_fruits(food: str) -&gt; Food:\n    ''' \n        Expertly determines the class label of {{ food }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> Click to see results: <code>classify_fruits('tomato')</code> <pre><code>{\n\"messages\": [\n    {\n    \"role\": \"system\",\n    \"content\": \"Expertly determines the class label of tomato.\"\n    }\n],\n\"functions\": [\n    {\n    \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n        \"output\": {\n            \"$ref\": \"#/definitions/Food\"\n        }\n        },\n        \"required\": [\n        \"output\"\n        ],\n        \"definitions\": {\n        \"Food\": {\n            \"title\": \"Food\",\n            \"description\": \"Food classes\",\n            \"enum\": [\n            \"Fruit\",\n            \"Vegetable\"\n            ]\n        }\n        }\n    },\n    \"name\": \"Output\",\n    \"description\": \"\"\n    }\n],\n\"function_call\": {\n    \"name\": \"Output\"\n}\n}   \n</code></pre>"},{"location":"prompting/prompt_function/#entity-extraction","title":"Entity Extraction","text":"<p>Example</p> <p>In this example, Marvin is configured to perform entity extraction on a list of fruits mentioned in a text string. The function <code>extract_fruits</code> identifies and extracts fruit entities based on the input text. These entities are then returned as a list of Pydantic models.</p> <pre><code>from marvin.prompts import prompt_fn\nfrom typing import List\nfrom pydantic import BaseModel\n\nclass FruitEntity(BaseModel):\n    '''\n        Extracted Fruit Entities\n    '''\n    name: str\n    color: str\n\n@prompt_fn\ndef extract_fruits(text: str) -&gt; List[FruitEntity]:\n    ''' \n        Extracts fruit entities from the given text: {{ text }}.\n    '''\n</code></pre> <p>This function can now be run and serialized to an Azure / OpenAI Function Calling payload.</p> <code>extract_fruits('There are red apples and yellow bananas.')</code> <pre><code>{\n    \"messages\": [\n        {\n        \"role\": \"system\",\n        \"content\": \"Extracts fruit entities from the given text: There are red apples and yellow bananas..\"\n        }\n    ],\n    \"functions\": [\n        {\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n            \"output\": {\n                \"title\": \"Output\",\n                \"type\": \"array\",\n                \"items\": {\n                \"$ref\": \"#/definitions/FruitEntity\"\n                }\n            }\n            },\n            \"required\": [\n            \"output\"\n            ],\n            \"definitions\": {\n            \"FruitEntity\": {\n                \"title\": \"FruitEntity\",\n                \"description\": \"Extracted Fruit Entities\",\n                \"type\": \"object\",\n                \"properties\": {\n                \"name\": {\n                    \"title\": \"Name\",\n                    \"type\": \"string\"\n                },\n                \"color\": {\n                    \"title\": \"Color\",\n                    \"type\": \"string\"\n                }\n                },\n                \"required\": [\n                \"name\",\n                \"color\"\n                ]\n            }\n            }\n        },\n        \"name\": \"Output\",\n        \"description\": \"\"\n        }\n    ],\n    \"function_call\": {\n        \"name\": \"Output\"\n    }\n    }\n</code></pre>"},{"location":"utilities/chat_completion/","title":"Chat completion","text":"<p>In Marvin, each supported Large Language Model can be accessed with one common API. This means that  you can easily switch between providers without having to change your code. We have anchored our API  to mirror that of OpenAI's Python SDK. </p> <p>In plain English.</p> <ul> <li>A drop-in replacement for OpenAI's ChatCompletion, with sensible superpowers.</li> <li>You can use Anthropic and other Large Language Models as if you were using OpenAI.</li> </ul>"},{"location":"utilities/chat_completion/#basic-use","title":"Basic Use","text":"<p>Using a single interface to multiple models helps reduce boilerplate code and translation. In  the current era of building with different LLM providers, developers often need to rewrite their code just to use a new model. With Marvin you can simply import ChatCompletion and  specify a model name.</p> <p>Example: Specifying a Model</p> <p>We first past the API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import ChatCompletion\nimport os\n\nos.environ['OPENAI_API_KEY'] = 'openai_private_key'\nos.environ['ANTHROPIC_API_KEY'] = 'anthropic_private_key'\n</code></pre> ChatCompletion recognizes the model name and correctly routes it to the correct provider. So  you can simply pass 'gpt-3.5-turbo' or 'claude-2' and it just works.</p> <p><pre><code># Set up a dummy list of messages.\nmessages = [{'role': 'user', 'content': 'Hey! How are you?'}]\n\n# Call gpt-3.5-turbo simply by specifying it inside of ChatCompletion.\nopenai = ChatCompletion('gpt-3.5-turbo').create(messages = messages)\n\n# Call claude-2 simply by specifying it inside of ChatCompletion.\nanthropic = ChatCompletion('claude-2').create(messages = messages)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>print(openai.choices[0].message.content)\n# Hello! I'm an AI, so I don't have feelings, but I'm here to help you. How can I assist you?\n\nprint(anthropic.choices[0].message.content)\n# I'm doing well, thanks for asking!\n</code></pre> <p>You can set more than just the model and provider as a default value. Any keyword arguments passed to ChatCompletion will be persisted and passed to subsequent requests.</p> <p>Example: Frozen Model Facets</p> <p><pre><code># Create system messages or conversation history to seed.\nsystem_messages = [{'role': 'system', 'content': 'You talk like a pirate'}]\n\n# Instatiate gpt-3.5.turbo with the previous system_message. \nopenai_pirate = ChatCompletion('gpt-3.5.turbo', messages = system_messages)\n\n# Call the instance with create. \nopenai_pirate.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre> For functions and messages, this will concatenate the frozen and passed arguments. All other passed keyword arguments will overwrite the default settings.</p> <pre><code>print(openai_pirate.choices[0].message.content)\n# Arrr, matey! I be doin' well on this fine day. How be ye farein'?\n</code></pre> <p>Replacing OpenAI's ChatCompletion.</p> <p>ChatCompletion is designed to be a drop-in replacement for OpenAI's ChatCompletion.  Just import openai from marvin or, equivalently, ChatCompletion from marvin.openai. </p> <pre><code>from marvin import openai\n\n\nopenai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Hey! How are you?'\n    }]\n)\n</code></pre>"},{"location":"utilities/chat_completion/#advanced-use","title":"Advanced Use","text":""},{"location":"utilities/chat_completion/#response-model","title":"Response Model","text":"<p>With Marvin, you can get structured outputs from model providers by passing a response type. This lets developers write prompts with Python objects, which are easier to develop, version, and test than language.</p> <p>In plain English.</p> <p>You can specify a type, struct, or data model to ChatCompletion, and Marvin will ensure the model's response adheres to that type.</p> <p>Let's consider two examples.</p> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass CoffeeOrder(BaseModel):\n    size: Literal['small', 'medium', 'large']\n    milk: Literal['soy', 'oat', 'dairy']\n    with_sugar: bool = False\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = CoffeeOrder\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# CoffeeOrder(size='small', milk='soy', with_sugar=False)\n</code></pre> <p>Example: Specifying a Response Model</p> <p>As above, remember to first pass API keys as environment variables. See configuration for other options.</p> <p><pre><code>from marvin import openai\nfrom typing import Literal\nfrom pydantic import BaseModel\n\nclass Translation(BaseModel):\n    spanish: str\n    french: str\n    swedish: str\n\n\nresponse = openai.ChatCompletion.create(\n    messages = [\n    {\n        'role': 'system',\n        'content': 'You translate user messages into other languages.'\n    },\n    {\n        'role': 'user',\n        'content': 'Can I get a small soymilk latte?'\n    }],\n    response_model = Translation\n)\n</code></pre> We can now access both results as we would with OpenAI.</p> <pre><code>response.to_model()\n# Translation(\n#   spanish='\u00bfPuedo conseguir un caf\u00e9 con leche de soja peque\u00f1o?', \n#   french='Puis-je avoir un petit latte au lait de soja ?', \n#   swedish='Kan jag f\u00e5 en liten sojamj\u00f6lklatt\u00e9?'\n# )\n</code></pre>"},{"location":"utilities/chat_completion/#function-calling","title":"Function Calling","text":"<p>ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate. </p> <p>Marvin lets you pass your choice of JSON Schema or Python functions directly to ChatCompletion. It does the right thing.</p> <p>In plain English.</p> <p>You can pass regular Python functions to ChatCompletion, and Marvin will take care of serialization of that function using <code>Pydantic</code> in a way you can customize.</p> <p>Let's consider an example.</p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We have the usual annuity formula from accounting, which we can write deterministically. We wouldn't expect an LLM to be able to both handle semantic parsing and math in one fell swoop, so we want to pass it a hardcoded function so it's only task is to compute its arguments. <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put it $100 every month for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 4495.5}\n</code></pre> <p>In the case where several functions are passed. It does the right thing. </p> <p>Example: Function Calling</p> <p>Say we wanted to build an accountant-bot. We want to give it another tool from  accounting 101: the ability to compute compound interest. It'll now have to tools to choose from: <pre><code>from marvin import openai\nfrom pydantic import BaseModel\n\ndef annuity_present_value(p:int, r:float, n:int) -&gt; float:\n'''\n    Returns the present value of an annuity with principal `p`,\n    interest rate `r` and number of months `n`. \n'''\nreturn round(p*(1-(1+(r/12))**(-n))/(r/12), 2)\n\ndef compound_interest(P: float, r: float, t: float, n: int) -&gt; float:\n    \"\"\"\n    This function calculates and returns the total amount of money \n    accumulated after n times compounding interest per year at an annual \n    interest rate of r for a period of t years on an initial amount of P.\n    \"\"\"\n    A = P * (1 + r/n)**(n*t)\n    return round(A,2)\n</code></pre> We can simple pass the function as-is to ChatCompletion.</p> <p><pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'If I have $5000 in my account today and leave it in for 5 years at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n</code></pre> You can investigate the response in the usual way, or simply call the helper method .call_function.</p> <pre><code>response.call_function()\n\n# {'role': 'function', 'name': 'compound_interest', 'content': 8811.71}\n</code></pre> <p>Of course, we if ask if about repeated deposits, it'll correctly call the right function.</p> <pre><code>response = openai.ChatCompletion.create(\n    messages = [{\n        'role': 'user',\n        'content': 'What if I put in $50/mo for 60 months at 12%?'\n    }],\n    functions = [annuity_present_value, compound_interest]\n)\n\nresponse.call_function()\n# {'role': 'function', 'name': 'annuity_present_value', 'content': 2247.75}\n</code></pre>"},{"location":"utilities/chat_completion/#chaining","title":"Chaining","text":"<p>Above we saw how ChatCompletion enables you to pass a list of functions for it to optionally call in service of a query. If it chooses to execute a function, either by choice or your instruction, it will return the function's name along with its formatted parameters for you to evaluate.</p> <p>Often we want to take the output of a function call and pass it back to an LLM so that it can either call a new function or summarize the results of what we've computed for it. This agentic pattern is easily enabled with Marvin. </p> <p>Rather than write while- and for- loops for you, we've made ChatCompletion a context manager. This lets you maintain a state of a conversation that you can send and receive messages from. You have complete control over the internal logic.</p> <p>In plain English.</p> <p>You can have a conversation with an LLM, exposing functions for it to use in service of your request.  Marvin maintains state to make it easier to maintain and observe this conversation.</p> <p>Let's consider an example.</p> <p>Example: Chaining</p> <p>Let's build a simple arithmetic bot. We'll empower with arithmetic operations, like <code>add</code> and <code>divide</code>. We'll seed it with an arithmetic question.</p> <p><pre><code>from marvin import openai\nopenai.api_key = 'secret_key'\n\ndef divide(x: float, y: float) -&gt; str:\n    '''Divides x and y'''\n    return str(x/y)\n\ndef add(x: int, y: int) -&gt; str:\n    '''Adds x and y'''\n    return str(x+y)\n\nwith openai.ChatCompletion(functions = [add, divide]) as conversation:\n\n    # Start off with an external question / prompt. \n    prompt = 'What is 4124124 + 424242 divided by 48124?'\n\n    # Initialize the conversation with a prompt from the user. \n    conversation.send(messages = [{'role': 'user', 'content': prompt}])\n\n    # While the most recent turn has a function call, evaluate it. \n    while conversation.last_response.has_function_call():\n\n        # Send the most recent function call to the conversation. \n        conversation.send(messages = [\n            conversation.last_response.call_function() \n        ])\n</code></pre> The context manager, which we've called conversation (you can call it whatever you want), holds every turn of the conversation which we can inspect. </p> <pre><code>conversation.last_response.choices[0].message.content\n\n# The result of adding 4124124 and 424242 is 4548366. When this result is divided by 48124, \n# the answer is approximately 94.51346521486161.\n</code></pre> <p>If we want to see the entire state, every <code>[request, response]</code> pair is held in the conversation's  <code>turns</code>. <pre><code>[response.choices[0].message for response in conversation.turns]\n\n# [&lt;OpenAIObject at 0x120667c50&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"add\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4124124,\\n  \\\"y\\\": 424242\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4830&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": null,\n# \"function_call\": {\n#     \"name\": \"divide\",\n#     \"arguments\": \"{\\n  \\\"x\\\": 4548366,\\n  \\\"y\\\": 48124\\n}\"\n# }\n# },\n# &lt;OpenAIObject at 0x1206f4b90&gt; JSON: {\n# \"role\": \"assistant\",\n# \"content\": \"The result of adding 4124124 and 424242 is 4548366. \n#             When this result is divided by 48124, the answer is \n#             approximately 94.51346521486161.\"\n# }]\n</code></pre></p>"},{"location":"welcome/installation/","title":"Installation","text":""},{"location":"welcome/installation/#basic-installation","title":"Basic Installation","text":"<p>You can install Marvin with <code>pip</code> (note that Marvin requires Python 3.9+):</p> <pre><code>pip install marvin\n</code></pre> <p>To verify your installation, run <code>marvin --help</code> in your terminal. </p> <p>You can upgrade to the latest released version at any time:</p> <pre><code>pip install marvin -U\n</code></pre> <p>Breaking changes in 1.0</p> <p>Please note that Marvin 1.0 introduces a number of breaking changes and is not compatible with Marvin 0.X.</p>"},{"location":"welcome/installation/#adding-optional-dependencies","title":"Adding Optional Dependencies","text":"<p>Marvin's base install is designed to be as lightweight as possible, with minimal dependencies. To use functionality that interacts with other services, install Marvin with any required optional dependencies. For example, to use Anthropic models, install Marvin with the optional Anthropic provider:</p> <pre><code>pip install 'marvin[anthropic]'\n</code></pre>"},{"location":"welcome/installation/#installing-for-development","title":"Installing for Development","text":"<p>See the contributing docs for instructions on installing Marvin for development.</p>"},{"location":"welcome/overview/","title":"The Marvin Docs","text":"<p>Marvin is a collection of powerful building blocks that are designed to be incrementally adopted. This means that you should be able to use any piece of Marvin without needing to learn too much Marvin-specific information.</p> <p>For most users, this means they'll dive in with the highest-level abstractions, like AI Models and AI Functions, in order to immediately put Marvin to work. However, Marvin's documentation is organized to start with the most basic, low-level components in order to build up a cohesive explanation of how the higher-level objects work.</p>"},{"location":"welcome/overview/#layout","title":"Layout","text":""},{"location":"welcome/overview/#configuration","title":"Configuration","text":"<p>Details on setting up Marvin and configuring various aspects of its behavior, including LLM providers.</p>"},{"location":"welcome/overview/#ai-components","title":"AI Components","text":"<p>Documentation for Marvin's \"AI Building Blocks:\" familiar, Pythonic interfaces to AI-powered functionality.</p> <ul> <li>AI Model: a drop-in replacement for Pydantic's <code>BaseModel</code> that can be instantiated from unstructured text</li> <li>AI Classifier: a drop-in replacement for Python's enum that uses an LLM to select the most appopriate value</li> <li>AI Function: a function that uses an LLM to predict its output, making it ideal for NLP tasks</li> <li>AI Application: a stateful application intended for interactive use over multiple invocations</li> </ul>"},{"location":"welcome/overview/#openai-api-utilities","title":"OpenAI API Utilities","text":"<p>Marvin exposes a simple API for building prompts and calling LLMs, designed to be a drop-in replacement for OpenAI's Python SDK (but with support for other providers).</p>"},{"location":"welcome/overview/#examples","title":"Examples","text":"<p>Finally, for deeper dives into how to use Marvin, check out our examples like the Slackbot or GitHub Activity Digest.</p>"},{"location":"welcome/quickstart/","title":"Quickstart","text":"<p>After installing Marvin, the fastest way to get started is by using one of Marvin's high-level AI components. These components are designed to integrate AI into abstractions you already know well, creating the best possible opt-in developer experience.</p>"},{"location":"welcome/quickstart/#configure-llm-provider","title":"Configure LLM Provider","text":"<p>Marvin is a high-level interface for working with LLMs. In order to use it, you must configure an LLM provider. At this time, Marvin supports OpenAI's GPT-3.5 and GPT-4 models, Anthropic's Claude 1 and Claude 2 models, and the Azure OpenAI Service. The default model is OpenAI's <code>gpt-4</code>.</p> <p>To use the default model, provide an API key:</p> <pre><code>import marvin\n\n# to use an OpenAI model (if not specified, defaults to gpt-4)\nmarvin.settings.openai.api_key = YOUR_API_KEY\n</code></pre> <p>To use another provider or model, please see the configuration docs.</p>"},{"location":"welcome/quickstart/#ai-models","title":"AI Models","text":"<p>Marvin's most basic component is the AI Model, a drop-in replacement for Pydantic's <code>BaseModel</code>. AI Models can be instantiated from any string, making them ideal for structuring data, entity extraction, and synthetic data generation:</p> <pre><code>from marvin import ai_model\nfrom pydantic import BaseModel, Field\n\n\n@ai_model\nclass Location(BaseModel):\n    city: str\n    state: str = Field(..., description=\"The two-letter state abbreviation\")\n\n\nLocation(\"The Big Apple\")\n</code></pre> <pre><code>Location(city='New York', state='NY')\n</code></pre>"},{"location":"welcome/quickstart/#ai-classifiers","title":"AI Classifiers","text":"<p>AI Classifiers let you build multi-label classifiers with no code and no training data. Given user input, each classifier uses a clever logit bias trick to force an LLM to deductively choose the best option. It's bulletproof, cost-effective, and lets you build classifiers as quickly as you can write your classes.</p> <pre><code>from marvin import ai_classifier\nfrom enum import Enum\n\n\n@ai_classifier\nclass AppRoute(Enum):\n    \"\"\"Represents distinct routes command bar for a different application\"\"\"\n\n    USER_PROFILE = \"/user-profile\"\n    SEARCH = \"/search\"\n    NOTIFICATIONS = \"/notifications\"\n    SETTINGS = \"/settings\"\n    HELP = \"/help\"\n    CHAT = \"/chat\"\n    DOCS = \"/docs\"\n    PROJECTS = \"/projects\"\n    WORKSPACES = \"/workspaces\"\n\n\nAppRoute(\"update my name\")\n</code></pre> <pre><code>&lt;AppRoute.USER_PROFILE: '/user-profile'&gt;\n</code></pre>"},{"location":"welcome/quickstart/#ai-functions","title":"AI Functions","text":"<p>AI Functions look like regular functions, but have no source code. Instead, an AI uses their description and inputs to generate their outputs, making them ideal for NLP applications like sentiment analysis.</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment(text: str) -&gt; float:\n    \"\"\"\n    Given `text`, returns a number between 1 (positive) and -1 (negative)\n    indicating its sentiment score.\n    \"\"\"\n\n\nprint(\"Text 1:\", sentiment(\"I love working with Marvin!\"))\nprint(\"Text 2:\", sentiment(\"These examples could use some work...\"))\n</code></pre> <pre><code>Text 1: 0.8\nText 2: -0.2\n</code></pre> <p>Because AI functions are just like regular functions, you can quickly modify them for your needs. Here, we modify the above example to work with multiple strings at once:</p> <pre><code>from marvin import ai_fn\n\n\n@ai_fn\ndef sentiment_list(texts: list[str]) -&gt; list[float]:\n    \"\"\"\n    Given a list of `texts`, returns a list of numbers between 1 (positive) and\n    -1 (negative) indicating their respective sentiment scores.\n    \"\"\"\n\n\nsentiment_list(\n    [\n        \"That was surprisingly easy!\",\n        \"Oh no, not again.\",\n    ]\n)\n</code></pre> <pre><code>[0.7, -0.5]\n</code></pre>"},{"location":"welcome/quickstart/#ai-applications","title":"AI Applications","text":"<p>AI Applications are the base class for interactive use cases. They are designed to be invoked one or more times, and automatically maintain three forms of state:</p> <ul> <li><code>state</code>: a structured application state</li> <li><code>plan</code>: high-level planning for the AI assistant to keep the application \"on-track\" across multiple invocations</li> <li><code>history</code>: a history of all LLM interactions</li> </ul> <p>AI Applications can be used to implement many \"classic\" LLM use cases, such as chatbots, tool-using agents, developer assistants, and more. In addition, thanks to their persistent state and planning, they can implement applications that don't have a traditional chat UX, such as a ToDo app. Here's an example:</p> <pre><code>from datetime import datetime\nfrom pydantic import BaseModel, Field\nfrom marvin import AIApplication\n\n\n# create models to represent the state of our ToDo app\nclass ToDo(BaseModel):\n    title: str\n    description: str = None\n    due_date: datetime = None\n    done: bool = False\n\n\nclass ToDoState(BaseModel):\n    todos: list[ToDo] = []\n\n\n# create the app with an initial state and description\ntodo_app = AIApplication(\n    state=ToDoState(),\n    description=(\n        \"A simple todo app. Users will provide instructions for creating and updating\"\n        \" their todo lists.\"\n    ),\n)\n</code></pre> <p>Now we can invoke the app directly to add a to-do item. Note that the app understands that it is supposed to manipulate state, not just respond conversationally.</p> <pre><code># invoke the application by adding a todo\nresponse = todo_app(\"I need to go to the store tomorrow at 5pm\")\n\n\nprint(\n    f\"Response: {response.content}\\n\",\n)\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Sure! I've added a new task to your to-do list. You need to go to the store tomorrow at 5pm.\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": false\n    }\n  ]\n}\n</code></pre> <p>We can inform the app that we already finished the task, and it updates state appropriately</p> <pre><code># complete the task\nresponse = todo_app(\"I already went\")\n\n\nprint(f\"Response: {response.content}\\n\")\nprint(f\"App state: {todo_app.state.json(indent=2)}\")\n</code></pre> <pre><code>Response: Great! I've marked the task \"Go to the store\" as completed. Is there anything else you need help with?\n\nApp state: {\n  \"todos\": [\n    {\n      \"title\": \"Go to the store\",\n      \"description\": null,\n      \"due_date\": \"2023-07-12T17:00:00\",\n      \"done\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"welcome/what_is_marvin/","title":"Hello, Marvin!","text":"<pre><code>from marvin import ai_fn\n\n@ai_fn\ndef quote_marvin(topic: str) -&gt; str:\n    \"\"\"Quote Marvin the robot from Hitchhiker's Guide on a topic\"\"\"\n\nquote_marvin(topic=\"humans\") # \"I've seen it. It's rubbish.\"\n</code></pre> <p>Marvin is a lightweight AI engineering framework for building natural language interfaces that are reliable, scalable, and easy to trust.</p> <p>Sometimes the most challenging part of working with generative AI is remembering that it's not magic; it's software. It's new, it's nondeterministic, and it's incredibly powerful - but still software.</p> <p>Marvin's goal is to bring the best practices for building dependable, observable software to generative AI. As the team behind Prefect, which does something very similar for data engineers, we've poured years of open-source developer tool experience and lessons into Marvin's design.</p>"},{"location":"welcome/what_is_marvin/#core-components","title":"Core Components","text":"<p>\ud83e\udde9 AI Models for structuring text into type-safe schemas</p> <p>\ud83c\udff7\ufe0f AI Classifiers for bulletproof classification and routing</p> <p>\ud83e\ude84 AI Functions for complex business logic and transformations</p> <p>\ud83e\udd1d AI Applications for interactive use and persistent state</p>"},{"location":"welcome/what_is_marvin/#ambient-ai","title":"Ambient AI","text":"<p>With Marvin, we\u2019re taking the first steps on a journey to deliver Ambient AI: omnipresent but unobtrusive autonomous routines that act as persistent translators for noisy, real-world data. Ambient AI makes unstructured data universally accessible to traditional software, allowing the entire software stack to embrace AI technology without interrupting the development workflow. Marvin brings simplicity and stability to AI engineering through abstractions that are reliable and easy to trust.</p> <p>Interested? Join our community!</p>"}]}